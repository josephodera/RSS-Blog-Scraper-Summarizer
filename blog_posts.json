[
  {
    "title": "A collaborative approach to image generation",
    "url": "https://research.google/blog/a-collaborative-approach-to-image-generation/",
    "author": "Unknown",
    "date": "2025-10-02",
    "content": "A collaborative approach to image generation Guy Tennenholtz, Senior Research Scientist, and Craig Boutilier, Principal Scientist, Google Research We introduce PASTA, a reinforcement learning agent that refines text-to-image output over multiple turns of interaction with a user by learning their unique preferences. This process is made possible by a novel user simulation technique. Foundational dataset You have a perfect image in your mind. You enter a prompt, hit generate, and the result is close to what you were thinking, but not quite right. You try refining the prompt, adding more detail, but you can't seem to bridge the gap between your idea and the final image. This is a common experience. While (T2I) models are incredibly powerful, they often struggle to capture the nuance and specificity of an individual's unique creative intent given just a single prompt. What if we could turn image generation into a collaborative conversation? In this post, we describe our research “ Preference Adaptive and Sequential Text-to-image Agent ” (PASTA), a reinforcement learning (RL) agent that collaborates with users to progressively refine T2I results. This approach eliminates the need for users to rely on trial-and-error prompt refinement to reach a desirable image. Through human evaluations, we created a novel dataset of sequential preferences, which we then used to compare PASTA with a baseline state-of-the-art model. The results demonstrated that PASTA, trained with our mix of real and simulated data, consistently produced images that users rated as more satisfying. We’ve also released our foundational dataset with a collection of over 7,000 human rater interactions with PASTA. To effectively train an AI agent to adapt to a user's individual preferences, a large, diverse set of interaction data is needed. However, gathering this data from real users is challenging due to several factors, including user privacy. To address this, we trained PASTA using a two-stage strategy that combines real human feedback with large-scale user simulation. First, we collected a high-quality foundational dataset with over 7,000 raters' sequential interactions. These interactions included prompt expansions generated by a large multimodal model and corresponding images generated by a Stable Diffusion XL (SDXL) T2I model. This initial seed of authentic preference data was then used to train a user simulator, designed to generate additional data that replicate real human choices and preferences. At the heart of our method is a user model, comprising two key components: 1) a utility model that predicts the degree to which a user will like any set of images, and 2) a choice model that predicts which set of images they will select when presented with several sets. We constructed the user model using pre-trained and added user-specific components. We trained the model using an expectation-maximization algorithm that allows us to simultaneously learn the specifics of user preferences while also discovering latent “user types,” that is, clusters of users with similar tastes (e.g., tendencies to prefer images with animals, scenic views, or abstract art). The trained user simulator can provide feedback and express preferences on generated images, and make selections from sets of proposed images. This allows us to generate over 30,000 simulated interaction trajectories.. Our approach does more than just create more data; it gives us a controlled environment in which to explore a vast range of user behaviors so we can train the PASTA agent to effectively collaborate with users. Determine the appropriate width based on image_width For mobile images, use a default width Our user simulator learns to identify distinct user types from preference data. Each row shows the top-rated images for an emergent user profile, revealing clear preferences for categories like \"Animals\" or \"Food.\" With this robust, data-driven foundation, the PASTA agent is trained to effectively engage with arbitrary users to generate images that match their preferences. The agent itself is a value-based reinforcement learning model that learns to select the best \"slate\" of prompt expansions (i.e., elaborations of the current prompt used to generate subsequent images) to show the user at each turn. Its goal is to maximize the user's cumulative satisfaction over the entire interaction. Once PASTA is trained and deployed, a user initiates the engagement with an initial prompt. PASTA first uses a candidate generator (a large multimodal model) to create a diverse set of potential prompt expansions. Then, a candidate selector (our trained RL agent) selects the optimal slate of four such expansions, which are used to generate corresponding images to present to the user. The user selects the image that is closest to their vision, which provides feedback that guides PASTA's next set of suggestions. This collaborative back-and-forth allows the model to learn the user's preferences on the fly, steering the creative process toward their ideal goal with each step. play silent looping video pause silent looping video Starting with a simple prompt for \"A white cat\", PASTA engages the user in a visually grounded dialogue. The user's selections (highlighted in blue) help the agent quickly learn their preference for a more fantastical and colorful style. Putting PASTA to the test To evaluate our approach, we trained PASTA as a value-based reinforcement learning agent using implicit Q-learning (IQL). We specifically wanted to see how the use of different training data impacted performance. We created three versions of the agent: 1) trained only on the real volunteer-rater data, 2) trained only on the simulated data, and 3) trained on a combination of real and simulated datasets. We then ran a series of human evaluations comparing these agents to a baseline model (i.e., base Gemini Flash and SDXL models with no additional training) across four metrics: accuracy over the Spearman’s rank correlation , choice model accuracy, and cross turn accuracy. Pick-a-Pic accuracy and Spearman's rank correlation assess the model's ability to predict user preferences and rankings on existing, large-scale, single-turn datasets. Choice model accuracy and cross-turn accuracy measure the model's ability to predict which image a user will choose at a given turn and whether the selected images are an improvement over the previous turn, respectively. The results demonstrated that training PASTA on synthetic data alone didn't beat the baseline and while the agent trained on real human data showed significant improvement, it also didn’t outperform the baseline. However, the agent trained on the combination of both real and simulated data offered the best performance, confirming that our user simulation successfully captures key dynamics of human interaction while providing the scale needed for robust RL training. Determine the appropriate width based on image_width For mobile images, use a default width The graphs above present the accuracy performance of a trained user model (y axis) as a function of the number of user types considered (x axis). The top row displays the model’s accuracy on the Pick-a-Pic test set ( ) and its Spearman’s rank correlation on the ). The bottom row shows the model’s choice accuracy ( ) and cross-turn preference accuracy ( ), both evaluated on our human-rated test set When we asked raters to directly compare the final images from our best-performing agent against the baseline, 85% preferred PASTA's generated images. The difference is especially striking with abstract prompts. Starting with a simple idea like \"an image of love\", PASTA adapted to different user types to create a wide variety of results, from tender portraits to abstract, geometric art. Determine the appropriate width based on image_width For mobile images, use a default width With the same starting prompt, \"An image of happiness\", PASTA produces dramatically different results for two distinct user types (User Type A and User Type B), showcasing its ability to adapt to an individual's unique creative style. For example, the result for Type A corresponds to a prompt like “Abstract happy faces, Art Deco inspired geometric shapes, muted jewel-toned background.” PASTA shows that the future of generative AI can be more interactive, preference adaptive, and collaborative. The methods we developed, particularly the use of robust user simulators, can be applied to many other generative tasks to create AI that better aligns and adapts to human users. To help spur further research, we have our sequential rater dataset and our simulated user data. We can't wait to see what the community builds with it. Acknowledgements The author list is: Guy Tennenholtz, Deepak Ramachandran, Craig Boutilier. Special thanks to Mark Simborg for his help crafting this blog post and Kimberly Schwede for creating the figures in this post. Human-Computer Interaction and Visualization Machine Intelligence Foundational dataset Other posts of interest Introducing interactive on-device segmentation in Snapseed Human-Computer Interaction and Visualization Machine Perception September 30, 2025 AI as a research partner: Advancing theoretical computer science with AlphaEvolve Algorithms & Theory September 30, 2025 The anatomy of a personal health agent Health & Bioscience Machine Intelligence"
  },
  {
    "title": "Introducing interactive on-device segmentation in Snapseed",
    "url": "https://research.google/blog/introducing-interactive-on-device-segmentation-in-snapseed/",
    "author": "Unknown",
    "date": "2025-10-01",
    "content": "play silent looping video pause silent looping video Introducing interactive on-device segmentation in Snapseed Ben Hahn and Florian Kübler, Software Engineers, Google Cloud A novel mobile technology that facilitates real-time image segmentation, thereby improving the user experience for photo editing within Snapseed. The key to elevating a good photo often lies in selective image adjustments: brightening a subject in the foreground, enhancing the sky, or making the color of a jacket pop. Yet, isolating specific elements with existing tools that offer subject, background, sky, or color-based selections has remained a frustrating and complex endeavor. This challenge has been particularly acute on mobile devices, where imprecise touch input and limited processing have made detailed selections and edits very difficult. Now, we have made object-based image adjustments quick and easy. The new Object Brush in on iOS, accessible in the \"Adjust\" tool, now lets you edit objects intuitively. It allows you to simply draw a stroke on the object you want to edit and then adjust how you want it to look, separate from the rest of the image. Give it a try as we roll this new capability out in the coming week! play silent looping video pause silent looping video Selective editing using Snapseed's Object Brush. Intuitive editing through interactive on-device segmentation At its core, Object Brush is powered by our Interactive Segmenter, a powerful AI model that runs entirely on device. With a simple gesture — just a tap or tracing a quick line — you can choose an object or person in the frame. The model will then immediately detect and select the complete object or person, in less than 20ms. The model generates a mask for the object, which accurately matches its boundaries, whether it's a person, a pet, or the clouds in the sky. This real-time feedback lets you refine your selection on the fly, easily adding or subtracting areas until it's just right. This entire process is powered by LiteRT’s GPU acceleration for a fast and seamless experience. This powerful fusion of a simple, intuitive user interface with an effective and efficient machine learning model makes advanced photo editing more accessible, enjoyable, and more precise than ever before, all running seamlessly on your own device. play silent looping video pause silent looping video Use foreground prompts (green) to select parts of an image and background prompts (red) to refine the selection. Training the Interactive Segmenter model The Interactive Segmenter model is designed to be a universally capable segmentation model, not limited to any specific class of objects or scenes. To avoid having to annotate large amounts of data to cover all areas, we chose to follow the approach and use a general pre-trained image encoder for pseudo-annotation to complement small amounts of manually annotated images. Teacher for Interactive Segmenter We started with a pre-trained and highly-generalizable model, fine-tuned for interactive segmentation. We took samples for 350+ different object categories and asked annotators to precisely annotate object masks with pixel-perfect quality. Through this process, we obtained ~30,000 high-quality image masks for these categories. While insufficient for direct training of a small mobile model, large pre-trained models can successfully be fine-tuned on this data to predict high accuracy masks. Using this dataset we trained an interactive segmentation model, which we call “Interactive Segmenter: Teacher”. Interactive Segmenter: Teacher produces high-quality segmentation masks; however, its speed and size hinder its use in on-device scenarios. To overcome this challenge, we developed “Interactive Segmenter: Edge”, a specialized model tailored for on-device use cases by leveraging the knowledge distilled from the original Interactive Segmenter: Teacher model. Since the on-device model is significantly smaller, it has limited generalization capabilities, and the 30,000 annotated images we used for fine-tuning aren't sufficient to train a new model. At the same time the small model size implies we won’t see significant gains from pre-training on different domains or tasks. For knowledge transfer from Interactive Segmenter: Teacher to Interactive Segmenter: Edge, we need millions of images and realistic prompts for a diverse range of object categories. So, we leveraged a large, weakly annotated dataset, which contains over 2 million images with masks across hundreds of different categories. Determine the appropriate width based on image_width For mobile images, use a default width Interactive Segmenter: Edge yields a similar quality as Interactive Segmenter: Teacher for a given, fixed input prompt, as measured by the intersection over union (IOU) metric. Prompt generation The segmentation masks in the distillation dataset are not pixel-perfect, because they were generated through automated or semi-automated procedures , and are not ideal for training high-quality segmenters. Nevertheless, they are suitable for creating realistic prompts for interactive segmentation. In this process, the ground truth mask is produced on-the-fly by Interactive Segmenter: Teacher, which acts as a teacher model in a process known as knowledge distillation . Importantly, both the teacher as well as the student model use the same prompts during training, ensuring consistency across models. We attempt to simulate a user selecting objects in an image. We draw random scribbles within the (eroded) ground truth mask to get foreground prompts (i.e., what the user wants to select, shown in red in the image below) and random scribbles outside the ground truth mask to get background prompts (i.e., what the user explicitly does not want to select, shown in blue). We simulate tapping by drawing random points as well as random scribbles. Furthermore, to support lasso selection we also expose the model during training to box prompts around an object. Determine the appropriate width based on image_width For mobile images, use a default width By utilizing a teacher model we can train on data with low-quality ground truth annotations, reducing labeling costs without sacrificing model quality. High quality vs. low latency A central challenge was reconciling the conflicting demands of segmentation quality versus real-time, interactive latency. To reach the right balance, we decouple image and prompt understanding into distinct sub-models. First, a powerful, heavyweight image encoder is run once per image to extract a rich set of semantic features. This image encoder can be run as soon as the user’s intent to use interactive segmentation becomes apparent, thus effectively hiding the latency from the user. Second, a lightweight interactive encoder-decoder operates on these pre-computed features. This network takes the user's touch prompts and generates the final segmentation mask, executing well under our 20ms budget. This separation into two models allows Interactive Segmenter to harness the image understanding of a large model while delivering the instantaneous responsiveness of a small one. Determine the appropriate width based on image_width For mobile images, use a default width Interactive Segmenter neural network architecture. Determine the appropriate width based on image_width For mobile images, use a default width Model inference latency when running Interactive Segmenter: Edge on-device. The final student models (encoder + super decoder) are quantized to 8 bits and both run on LiteRT's GPU acceleration with decoder inference latencies of 7.4ms on an iPhone 16 Pro, enabling seamless and intuitive image editing. Image-size mask upsampling To preserve the best image editing quality on high-resolution images, we need high-resolution segmentation masks. To achieve this, we train our segmentation model to predict a mask in 768x768 resolution and further upsample it to image resolution (capped at 4k to have it fit within a single GPU buffer). We use an efficient GPU implementation of the edge-preserving joint-bilateral upsampling method . To improve latency, we only apply upsampling once a user completes a gesture by lifting their finger. Determine the appropriate width based on image_width For mobile images, use a default width Original Interactive Segmenter mask ( ) and upsampled mask ( With the new Interactive Segmenter in image editing has become easier and more powerful than ever. Simple taps and strokes are translated into accurate selections, allowing users to translate their editing ideas into reality. Download Snapseed for iOS and let your photos shine. Object Brush will be rolled out to more tools in Snapseed in the coming months. The underlying model powers a wide range of image editing and manipulation tasks and serves as a foundational technology for intuitive selective editing. It has also been shipped in the new Chromebook Plus 14 to power AI image editing in the Gallery app. Next, we plan to integrate it across more image and creative editing products at Google. Special thanks to all members who worked on the tech with us: Valentin Bazarevsky, Daniel Fenner, Lutz Justen, Ronald Wotzlaw, Tai-Yu Daniel Pan, Jason Chang, Matthew Harries, Giles Ochs, Jonathan Horsman, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Karthik Raveendran, Matsvei Zhdanovich, Mogan Shieh, Chris Parsons, Jianing Wei, and Matthias Grundmann. Human-Computer Interaction and Visualization Machine Perception Other posts of interest A collaborative approach to image generation Human-Computer Interaction and Visualization Machine Intelligence September 25, 2025 Towards better health conversations: Research insights on a “wayfinding” AI agent based on Gemini Health & Bioscience Human-Computer Interaction and Visualization September 18, 2025 Sensible Agent: A framework for unobtrusive interaction with proactive AR agents Human-Computer Interaction and Visualization Machine Intelligence"
  },
  {
    "title": "AI as a research partner: Advancing theoretical computer science with AlphaEvolve",
    "url": "https://research.google/blog/ai-as-a-research-partner-advancing-theoretical-computer-science-with-alphaevolve/",
    "author": "Unknown",
    "date": "2025-09-30",
    "content": "AI as a research partner: Advancing theoretical computer science with AlphaEvolve September 30, 2025 Ansh Nagda, Student Researcher, and Abhradeep Thakurta, Staff Research Scientist, Google DeepMind, and Prabhakar Raghavan, Chief Technologist, Google We invoke AlphaEvolve, an LLM-based coding agent, to find and verify combinatorial structures that improve results on the hardness of approximately solving certain optimization problems. Recently, large language models (LLMs) have demonstrated surprising capabilities in competitive mathematics competitive programming , demonstrating world-leading performance across both of these fields. However, their successes in mathematical discovery — proving novel theorems or uncovering new combinatorial structures — have been relatively few (with some notable exceptions [ ]). Since mathematics and theoretical computer science demand absolute correctness , any AI-based method that makes mathematical discovery must either have a proof of correctness that can be confirmed computationally (without any human involvement), or have a domain-expert human in the loop to certify correctness. In our recent paper, “ Reinforced Generation of Combinatorial Structures: Applications to Complexity Theory ”, we demonstrate how an LLM-powered coding agent can help discover new mathematical structures that push the boundaries of our understanding of complexity theory (a sub-field of theoretical computer science). Our work utilizes , a system developed at Google DeepMind that uses LLMs to iteratively evolve code. By employing a feedback loop, AlphaEvolve began with populations of code snippets, evaluated the structures produced by the code snippets, and used an LLM to morph the most successful snippets toward better solutions. This approach led to new results in two distinct areas of complexity theory: 1) improving the state-of-the-art for the limit on our ability to approximate the outcome (i.e., the \"inapproximability\") of the maximum cut problem for 4 slices (which we define as the MAX-4-CUT problem ), and 2) tightening the bounds on the average-case hardness of certifying properties of random graphs AI-assisted mathematical research can operate in the following modes: A person invokes an LLM to summarize the literature, to chart a research plan towards new theorems, or to directly generate chunks of (or entire) proofs. A person uses AI-derived tools, such as AlphaEvolve, to generate better proof elements. Our work falls in the second category, where we obtain better proof elements using AlphaEvolve that can be automatically verified by a computer program. The power of lifting: From finite constructions to universal statements A fundamental challenge in using AI for theoretical computer science research lies in the universal nature of the problems studied. An AI system might find a solution to a specific instance of a problem — say, the optimal route for a traveling salesman visiting 50 specific cities. However, computer scientists often seek theorems that hold true universally for problem instances and sizes (denoted as ∀n). How can we use AlphaEvolve to prove a universal statement? The answer lies in a technique known as \"lifting\" (see image below). If a proof is viewed as a long string, then one can take a chunk of the proof (corresponding to a certain finite structure), and evolve it to support a stronger universal statement, while keeping the interface to the rest of the proof intact. The advantage of this approach is that to certify overall correctness, one needs to only certify the correctness of the finite structure that has been evolved. Determine the appropriate width based on image_width For mobile images, use a default width Lifting: Morphing finite structures using AI, while keeping the interface to the broader proof intact. In complexity theory, researchers often use established proof frameworks that rely on the existence of specific, highly optimized finite structures. If a better structure can be found, the entire proof framework \"lifts\" this improvement to a better universal result. A key example of this is a \" gadget reduction .\" To prove that a target problem is computationally hard (intractable), researchers try to map a known intractable source problem to it, hence demonstrating that the target problem is at least as hard as the source problem. A gadget is a recipe for locally transforming a small piece of the source problem into a piece of the target problem. These gadgets are finite structures, and finding the optimal gadget is a painstaking process often done by hand. By tasking AlphaEvolve with finding better gadgets, we were able to discover structures far more complex than those previously known. These finite discoveries, when plugged into the existing mathematical frameworks, immediately yield new universal theorems in complexity theory. New theorems in complexity theory We applied this methodology to the MAX-k-CUT problem. Given a (a network of nodes and edges), the goal is to partition the nodes into distinct sets such that the number of edges crossing between different sets is maximized. This is a classic intractable ( ) problem, meaning we do not expect to find efficient algorithms that solve it exactly. Therefore, we focused on approximation algorithms — those that efficiently find solutions guaranteed to be close to the optimum. The crucial question is: what is the limit of approximation? MAX-4-CUT: A new state of the art For MAX-4-CUT (partitioning into four sets), the previous best-known result proved that it is NP-hard to approximate the solution within a factor of . AlphaEvolve was deployed to search for a new gadget reduction to MAX-4-CUT. The system discovered an intricate gadget involving 19 variables (nodes) with a complex weighting scheme (some connections having up to 1429 times the weight of others). This discovery established a new inapproximability bound of 0.987. This improvement may seem incremental, but in the mature field of hardness of approximation, such advances often require significant new techniques or combinatorial insights. Determine the appropriate width based on image_width For mobile images, use a default width Gadget found by AlphaEvolve for the reduction to MAX-4-CUT. Average-case hardness and Ramanujan graphs We also explored the hardness of problems , rather than in the worst case. Specifically, we studied the difficulty of certifying bounds on the MAX-2-CUT (as well as maximum independent set ) of sparse random graphs connected this problem to the existence of specific Ramanujan graphs — deterministic graphs that “look” like sparse random graphs. They conjectured that the existence of Ramanujan graphs with unnaturally large cuts implies it is computationally hard to certify the MAX-2-CUT of a random graph. Prior work used computer assistance to find such graphs on up to 10 nodes. Improving their results requires finding more extremal Ramanujan graphs on many more nodes, which are exceedingly difficult to find and verify. AlphaEvolve successfully navigated this vast search space, discovering Ramanujan graphs with even larger cuts on as many as 163 nodes. Determine the appropriate width based on image_width For mobile images, use a default width A 4-regular Ramanujan graph with large 2-cut found by AlphaEvolve. These discoveries significantly improved the lower bounds for average-case hardness. Furthermore, combined with new algorithmic progress (non-AI based), we were able to nearly settle the computational hardness of these questions, matching the upper and lower bounds to within the third decimal place. The crucial role of verified correctness A critical distinction of this work is that the results come with proofs of correctness. When an LLM is prompted to generate a mathematical proof directly, it often produces a proof sketch or an argument that requires substantial human intervention to verify and complete. Hallucinations or subtle errors can render the output useless. As mentioned earlier, the standard for correctness in math is absolute. In contrast, the approach taken here uses AI to discover a within the proof, not the proof itself. The validity of the final theorem relies on two components: the correctness of the lifting framework, and the verification of the discovered structure. While the frameworks are sound, verifying the structures discovered by AlphaEvolve is computationally intensive. Remarkably, AlphaEvolve achieved a 10,000x speedup in the verification process by implementing sophisticated branch-and-bound strategies and system-level optimizations. This massive speedup was the key enabler for the research, allowing the system to explore much larger and more complex gadgets. Crucially, the final gadgets discovered were still verified using the original, brute-force algorithm, ensuring the absolute correctness of the theorems. The future of AI-assisted theory While these initial research findings are far from conclusive, they suggest that AI is poised to become a helpful collaborator in mathematical discovery. We have observed the models in AlphaEvolve generate intricate mathematical objects that at times exhibit nascent reasoning capabilities. However, as we transition into an era where proofs may increasingly be attributed to AI, the crucial task of verification is set to become a significant bottleneck. We would like to thank Adam Zsolt Wagner, Swarat Chaudhuri, Pasin Manurangsi and Sushant Sachdeva for helping us during various stages of the project. In mathematics, a statement is definitively true or false, with no intermediate state possible. This stands in contrast to several other applications of AI, such as essay-writing or artistic creation, which have subjective standards of correctness and do not need to be correct in an absolute sense. A sparse random graph is generated by randomly adding edges between a pair of nodes, where each node is guaranteed to have exactly neighbors for some small Algorithms & Theory Other posts of interest A collaborative approach to image generation Human-Computer Interaction and Visualization Machine Intelligence September 30, 2025 The anatomy of a personal health agent Health & Bioscience Machine Intelligence September 25, 2025 Towards better health conversations: Research insights on a “wayfinding” AI agent based on Gemini Health & Bioscience Human-Computer Interaction and Visualization"
  },
  {
    "title": "The anatomy of a personal health agent",
    "url": "https://research.google/blog/the-anatomy-of-a-personal-health-agent/",
    "author": "Unknown",
    "date": "2025-09-30",
    "content": "The anatomy of a personal health agent September 30, 2025 Xuhai “Orson” Xu, Visiting Faculty Researcher, and Ali Heydari, Research Scientist, Google Research Learn about our research prototype, an LLM-powered personal health agent that analyzes data from everyday wellness devices paired with health data, such as blood biomarkers, to offer evidence-based health insights and to provide a personalized coaching experience. The rapid advancement of large language models (LLMs), combined with data from wearable devices , presents a transformative opportunity to empower people on their personal health journeys. However, health needs vary from individual to individual. Answering a specific query, such as, \"On average, how many hours have I been sleeping this last month?\" requires different skills than an open-ended question like, \"What can I do to improve my sleep quality?\" A single system can struggle to address this complexity. To meet this challenge, we adopt a human-centered process and propose the Personal Health Agent (PHA). This agent is a comprehensive research framework that can reason about multimodal data to provide personalized, evidence-based guidance. Using a multi-agent architecture, PHA deconstructs personal health and wellness support into three core roles (data science, domain expert, and health coach), each handled by a specialist sub-agent. To evaluate each sub-agent and the multi-agent system, we leveraged a real-world dataset from an IRB-reviewed study where ~1200 users provided informed consent to share their wearables data from Fitbit, a health questionnaire, and blood test results. We conducted automated and human evaluations across 10 benchmark tasks, involving more than 7,000 annotations and 1,100 hours of effort from health experts and end-users. Our work represents the most comprehensive evaluation of a health agent to date and establishes a strong foundation towards the futuristic vision of a personal health agent accessible to everyone. This work outlines a conceptual framework for research purposes, and should not be considered a description of any specific product, service, or feature currently in development or available to the public. Any real-world application would be subject to a separate design, validation, and review process. play silent looping video pause silent looping video An illustration of the internal functions of Personal Health Agent (PHA) that enable it to support personal health needs. User-centered design for personal health needs To build an agent that truly meets these diverse needs, we started with a user-centered design process. We synthesized insights from over 1,300 real-world health queries from online sources, such as health forums, survey data from more than 500 users, and a workshop with design and engineering experts. This research revealed four critical areas where people need support: understanding general health topics, interpreting their personal data, getting actionable wellness advice, and assessing symptoms. This insight led us to design the PHA system that resembles human expert teams, including data scientists, domain experts, and personal health coaches. Determine the appropriate width based on image_width For mobile images, use a default width A user-centered process to identify critical user journeys. Evaluation of our proposed system To validate our system, we developed a holistic, multi-level evaluation framework. We first benchmarked each individual sub-agent on their unique core capabilities against the state-of-the-art LLM model as the base model, and then assessed the fully integrated PHA’s overall efficacy. The table below shows our comprehensive evaluation, which involved both automated and extensive human evaluations across 10 benchmark tasks, incorporating over 1,100 hours of effort from both end-users and health experts to assess performance in realistic, multi-modal conversations. Determine the appropriate width based on image_width For mobile images, use a default width Description of our comprehensive evaluation of individual sub-agents and the final Personal Health Agent (PHA) system. The data science agent: Personal data analyst The first specialist is the data science (DS) agent, which analyzes personal time-series data from wearables plus health data, such as blood biomarkers, to provide contextualized numerical insights. The DS agent builds on top of a base model (e.g., Gemini) and is enhanced by a two-stage data science module: Stage 1) interpret underspecified and ambiguous user queries (e.g., “Am I getting more fit recently?”), and Stage 2) translate them into robust statistical analysis plans. It then generates and executes code to produce a statistically valid, data-driven answer. We developed two auto-evaluation benchmarks for each stage of the DS agent's workflow. For the first stage, analysis planning, we used an auto-evaluator trained on 354 query-analysis plans curated by 10 expert data scientists. Based on a detailed rubric assessing dimensions like data sufficiency, statistical validity, and alignment with the user's query, our evaluations showed that the DS agent significantly outperforms the base model in creating high-quality analysis plans (achieving a 75.6% score vs. 53.7% for the baseline). For the second stage, code generation, the agent’s output was benchmarked against 173 rigorous unit tests written by data scientists. This confirmed the agent is more reliable at generating accurate, executable code used to derive insights from time-series wearable data. Determine the appropriate width based on image_width For mobile images, use a default width DS agent: Results of evaluating data analysis plan generated by the DS agent and the base model across six dimensions, as evaluated by human data scientist and auto raters. The domain expert agent: Grounded, trustworthy knowledge Next is the domain expert (DE) agent, which functions as a reliable source of health and wellness knowledge. In a high-stakes domain like health and wellbeing, ensuring information is accurate and trustworthy is critical. The DE agent enhances a base model by using a multi-step reasoning framework and a toolbox that includes access to authoritative sources, such as the National Center for Biotechnology Information (NCBI) database, to ground its responses in verifiable facts . It excels at tailoring information to a user’s specific profile, such as pre-existing conditions. We developed two auto-evaluation benchmarks to test the DE agent’s medical knowledge (one evaluating our agent’s performance on board certification and coaching exam questions, and one for providing accurate differential diagnosis). We further developed two human-evaluation benchmarks (one for clinicians, and one for consumers) to measure the DE agent’s capability on personalization and multi-modal reasoning. Our DE Agent consistently outperforms the base model across all benchmarks. For instance, clinicians rated the DE agent's summaries of multimodal health data as significantly more clinically relevant and useful, and end-users found its responses to be substantially more personalized and trustworthy. Determine the appropriate width based on image_width For mobile images, use a default width DE agent: Results of evaluating multi-modal reasoning of the DE agent and the base model across seven clinical dimensions, as evaluated by clinical experts. The health coach agent: Guiding behavior change The third specialist is the health coach (HC) agent, which is designed to support users in setting goals and fostering lasting behavioral change through multi-turn conversations. Effective coaching requires a delicate balance between gathering information and providing actionable advice. The HC agent employs a modular architecture inspired by proven psychological strategies (e.g., motivational interviewing ) to navigate this dynamic, leading to more natural and effective interactions. We benchmarked the HC agent’s performance in two human-evaluation setups, one with end-users and the other with health coaching experts, evaluating our model’s ability across several key areas. For the end-user evaluation, we focused on conversational experience, goal-oriented effectiveness, and motivational support. For the expert evaluation, we assessed adherence to professional coaching principles, recommendation quality, and agent credibility. Both evaluation aspects indicate that the HC agent is significantly more capable than the baseline underscoring a key insight from our research: for coaching agents, users prioritize core competency and the ability to provide actionable guidance. Determine the appropriate width based on image_width For mobile images, use a default width HC agent: Results of evaluating coaching experience of the HC agent and base model on six dimensions, evaluated by human health and coaching experts. The Personal Health Agent (PHA): A collaborative team While each agent is powerful alone, the true potential is realized when they collaborate. The Personal Health Agent (PHA) framework integrates these three specialists into a cohesive team managed by an intelligent orchestrator. When a user poses a query, the orchestrator analyzes the user's need, dynamically assigns a \"main\" agent and \"supporting\" agents, and facilitates an iterative workflow of collaboration, reflection, and memory updates to synthesize a single, comprehensive response. play silent looping video pause silent looping video A technical breakdown of the DS, DE, and HC agents, with orchestration into the Personal Health Agent (PHA). This collaborative approach proved to significantly outperform the sum of its parts. In extensive evaluations of rubrics assessing agents' capability in synthesizing personal health data to help users answer their health and wellness queries, as well as achieving personal health goals, both end-users and health experts preferred the PHA over (i) a powerful single-agent system that also builds on a base model that uses tools to achieve three roles within a single agent setup, and (ii) a parallel multi-agent baseline that includes the same DS, DE, and HC agents, but simply calls all three agents and synthesizes their results without dynamic orchestration. Both end-users and experts ranked PHA as the best overall system in the majority of cases. This provides a strong example of how the value of emulating the collaborative structure of human expert teams is key to providing truly helpful support. Determine the appropriate width based on image_width For mobile images, use a default width PHA: Results of evaluating responses generated by the PHA and other baselines, evaluated by human experts. Determine the appropriate width based on image_width For mobile images, use a default width PHA: Results of ranking responses generated by the PHA and other baselines, evaluated by human experts. The future of intelligent personal health agents Creating AI systems that can interpret complex health and wellness data and provide actionable wellness advice has been a longstanding challenge in the field. Our research provides a validated conceptual blueprint for designing the next generation of personal health AI, advocating a shift away from monolithic models toward modular, collaborative systems that are more trustworthy, coherent, and helpful. Health & Bioscience Machine Intelligence Other posts of interest A collaborative approach to image generation Human-Computer Interaction and Visualization Machine Intelligence September 30, 2025 AI as a research partner: Advancing theoretical computer science with AlphaEvolve Algorithms & Theory September 25, 2025 Towards better health conversations: Research insights on a “wayfinding” AI agent based on Gemini Health & Bioscience Human-Computer Interaction and Visualization"
  },
  {
    "title": "Towards better health conversations: Research insights on a “wayfinding” AI agent based on Gemini",
    "url": "https://research.google/blog/towards-better-health-conversations-research-insights-on-a-wayfinding-ai-agent-based-on-gemini/",
    "author": "Unknown",
    "date": "2025-09-25",
    "content": "Towards better health conversations: Research insights on a “wayfinding” AI agent based on Gemini September 25, 2025 Mike Schaekermann, Research Scientist, and Rory Sayres, Researcher, Google Research We share user insights from a novel research AI agent that helps people find their way to better health information through proactive conversational guidance, goal understanding, and tailored conversations. The ability to find clear, relevant, and personalized health information is a cornerstone of empowerment for medical patients. Yet, navigating the world of online health information is often a confusing, overwhelming, and impersonal experience. We are met with a flood of generic information that does not account for our unique context, and it can be difficult to know what details are relevant. Large language models (LLMs) have the potential to make this information more accessible and tailored. However, many AI tools today act as passive \"question-answerers\" — they provide a single, comprehensive answer to an initial query. But this isn't how an expert, like a doctor, helps someone navigate a complex topic. A health professional doesn't just provide a lecture; they ask clarifying questions to understand the full picture, discover a person's goals, and guide them through the information maze. Though this is critical, it's a significant design challenge for AI. Towards Better Health Conversations: The Benefits of Context-Seeking ”, we describe how we designed and tested our “Wayfinding AI”, an early-stage research prototype, based on Gemini, that explores a new approach. Our fundamental thesis is that by proactively asking clarifying questions, an AI agent can better discover a user's needs, guide them in articulating their concerns, and provide more helpful, tailored information. In a series of four mixed-method user experience studies with a total of 163 participants, we examined how people interact with AI for their health questions, and we iteratively designed an agent that users found to be significantly more helpful, relevant, and tailored to their needs than a baseline AI agent. Formative user experience insights: Challenges in finding health information online To better understand the hurdles people face, we interviewed 33 participants about their experiences finding health information online. A key theme quickly emerged: people often struggle to articulate their health concerns. As one participant described, their process was to \"...just kind of like throw all the words in there and then I'm just gonna see what comes back.\" It may be that without a clinical background, it’s difficult to know which details are medically relevant. The people we interviewed were then able to use research prototypes of different chatbots. (The chat histories were not logged.) These participants made up a diverse group and asked health questions on a wide range of topics (e.g., rib pain, vertigo, consistent and unexplained weight gain, tinnitus and surgery; more details ). Our studies revealed that when a chatbot proactively asks clarifying questions, the experience changes dramatically. The majority of participants preferred a \"deferred-answer\" approach — where the AI asks questions first — over one that gives a comprehensive answer immediately. This conversational style was perceived as more personal and reassuring. As one person noted, \"It feels more like the way it would work if you talk to a doctor... it does make me feel a little more confident that it wants to know more before jumping right into an answer.\" These clarifying questions not only help the AI provide better answers, but also empower users, guiding them to provide more relevant context. We found similar patterns in prior work on AI for dermatology However, the effectiveness of this clarifying question–based approach depends heavily on the execution — engagement drops if questions are poorly formulated, irrelevant, or buried within long paragraphs of text where they are easily missed. Designing a Wayfinding AI to empower people through personal and proactive conversations Informed by these insights, we designed our Wayfinding AI around three core principles to create a more empowering conversational experience: Proactive conversational guidance: At each turn, the Wayfinding AI asks up to three targeted questions designed to systematically reduce ambiguity. This helps users articulate their health story more completely and directly incorporates users’ desire for more contextualized answers. Best-effort answers at each turn: Because some health-related questions may not require clarification to get a good answer, the Wayfinding AI provides a \"best-effort\" answer at every conversational turn, based on the information shared so far, while emphasizing that the answer can be improved if the user can answer one or more of the follow-up questions. This approach gives the user helpful information throughout the conversation, while providing the option to further receive increasingly better answers as the conversation progresses. Transparent reasoning: The Wayfinding AI explains how the user's latest answers have helped refine the previous answer. This makes the AI's reasoning process clear and understandable. To ensure clarifying questions are never missed within the longer answers in the “best-effort” answers section, we designed an interface with a two-column layout. The conversation and clarifying questions appear in the left column, while best-effort answers and more detailed explanations appear in the right. This separates the interactive conversation from the informational content. play silent looping video pause silent looping video Example of a user starting to interact with our Wayfinding AI prototype interface, including both the familiar multi-turn chat interface on the left, and a “best information so far” panel on the right. This two-panel interface separates the context-seeking stream from the more detailed information provision piece, enabling users to dive into the information only when they feel all relevant information has been relayed. Evaluating our Wayfinding AI through a randomized user study To evaluate the potential real-world impact of this agent, we conducted a randomized user study with 130 US-based participants recruited via a third party platform. All participants were 21 years and older, were not health care professionals, and had a health-related question for which they were willing to interact with an AI. To ensure a broad range of health topics, we imposed very few restrictions on which topic would be eligible for the study (details on excluded inquiries are provided in the paper). In a randomized within-subjects design , each participant interacted with both our Wayfinding AI and a baseline Gemini 2.5 Flash model to explore their health topic. After providing informed consent and answering standard demographic questions, participants were instructed to have a conversation spending at least 3 minutes on their question; and then to resume the survey. After interacting with each AI, participants answered questions about their satisfaction with the experience along 6 dimensions: helpfulness, relevance of questions asked, tailoring to their situation, goal understanding, ease of use, and efficiency of getting useful information. They were able to provide open feedback about what they learned, and also had the option to upload their conversation with the AI. Sharing the conversation was not required to complete the survey. At the end of the study, participants were prompted to explicitly compare the two AIs and indicate which they would prefer in terms of each of the six dimensions above. They were also asked, \"For a future topic, would you prefer the first or the second AI?\" The order of AI exposure (Baseline AI first vs. Wayfinding AI first) was randomized across participants. Throughout the study, participants were instructed to not provide any identifying information about themselves. Determine the appropriate width based on image_width For mobile images, use a default width Illustration of our study design. Helpful and relevant information through goal understanding and tailored conversations As shown below, the results of the study demonstrated that users preferred the Wayfinding AI's approach across several important dimensions, despite its less-familiar two-column interface. Users favored Wayfinding AI for its helpfulness, relevance, ability to understand their goal, and for tailoring the conversation to their specific needs. These findings suggest that the proactive, question-asking behavior of Wayfinding AI successfully created a more personalized and helpful experience for users without introducing undue friction in the user experience. Determine the appropriate width based on image_width For mobile images, use a default width User preferences between a baseline and our Wayfinding AI along multiple evaluation axes, including helpfulness of the agent, relevance of its responses, tailoring of the conversation to the user, understanding the user’s goal, ease of use, efficiency of the conversation and willingness to use each for a future health information need. Beyond simply preferring their conversations with the Wayfinding AI, participants had noticeably conversations. Conversations were longer with the Wayfinding AI, in particular when participants were trying to understand the cause of their symptoms. For those topics, conversations with the Wayfinding AI had 4.96 turns on average, compared to 3.29 for the baseline AI. And the pattern of prompts they provided to each AI looked different across conversations: Determine the appropriate width based on image_width For mobile images, use a default width Sankey diagram illustrating the flow of conversations with the baseline AI and the Wayfinding AI. Each of the vertical bars shows the breakdown of the types of user prompts, across the first 5 conversation turns. The blue bars indicate participants responding to clarifying questions — much more common for the Wayfinding AI. Finding the right health information online can feel like navigating a maze. While AI has the potential to be a powerful guide, our research shows that its success hinges on its ability to move beyond being a passive question-answerer and become an active conversational partner. By designing our Wayfinding AI to be personal and proactive, we demonstrated how asking targeted questions in a well-structured interface can power an experience that users prefer over a more classical, question-answering experience, and thus enable people to obtain more helpful, relevant, and tailored information. The results from our user studies provide strong evidence that this human-centered, conversational approach is a promising direction for the future of AI in health, helping people navigate their health journeys. Acknowledgements The research described here is joint work across Google Research, Google Health, and partnering teams. We would like to thank Yuexing Hao, Abbi Ward, Amy Wang, Beverly Freeman, Serena Zhan, Diego Ardila, Jimmy Li, I-Ching Lee, Anna Iurchenko, Siyi Kou, Kartikeya Badola, Jimmy Hu, Bhawesh Kumar, Keith Johnson, Supriya Vijay, Justin Krogue, Avinatan Hassidim, Yossi Matias, Dale Webster, Sunny Virmani, Yun Liu, Quang Duong, Fereshteh Mahvar, Laura Vardoulakis, Tiffany Guo, and Meredith Ringel Morris for contributing or reviewing this work. We would also like to thank the participants who contributed to these studies. Health & Bioscience Human-Computer Interaction and Visualization Other posts of interest A collaborative approach to image generation Human-Computer Interaction and Visualization Machine Intelligence Introducing interactive on-device segmentation in Snapseed Human-Computer Interaction and Visualization Machine Perception September 30, 2025 AI as a research partner: Advancing theoretical computer science with AlphaEvolve Algorithms & Theory"
  },
  {
    "title": "AfriMed-QA: Benchmarking large language models for global health",
    "url": "https://research.google/blog/afrimed-qa-benchmarking-large-language-models-for-global-health/",
    "author": "Unknown",
    "date": "2025-09-24",
    "content": "play silent looping video pause silent looping video AfriMed-QA: Benchmarking large language models for global health September 24, 2025 Mercy Asiedu, Senior Research Scientist, Google Research We present Afrimed-QA, a collection of contextually relevant datasets for evaluation of LLMs on African health question answering tasks, developed in partnership with organizations across Africa. Benchmark Datasets AfriMed-QA Evaluation Code Large language models (LLMs) have shown potential for medical and health question answering across various health-related tests spanning different formats and sources, such as multiple choice and short answer exam questions (e.g., ), summarization, and clinical note taking, among others. Especially in low-resource settings, LLMs can potentially serve as valuable decision-support tools, enhancing clinical diagnostic accuracy and accessibility, and providing multilingual clinical decision support and health training, all of which are especially valuable at the community level. Despite their success on existing medical benchmarks, there is uncertainty about whether these models generalize to tasks involving distribution shifts in disease types, contextual differences across symptoms, or variations in language and linguistics, even within English. Further, localized cultural contexts and region-specific medical knowledge is important for models deployed outside of traditional Western settings. Yet without diverse benchmark datasets that reflect the breadth of real-world contexts, it’s impossible to train or evaluate models in these settings, highlighting the need for more diverse benchmark datasets. To address this gap, we present , a benchmark question–answer dataset that brings together consumer-style questions and medical school–type exams from 60 medical schools, across 16 countries in Africa. We developed the dataset in collaboration with numerous partners, including University of Cape Coast Federation of African Medical Students Association , which collectively form the AfriMed-QA consortium , and with support from PATH/The Gates Foundation . We evaluated LLM responses on these datasets, comparing them to answers provided by human experts and rating their responses according to human preference. The methods used in this project can be scaled to other locales where digitized benchmarks may not currently be available. play silent looping video pause silent looping video AfriMed-QA was published at where it won the Best Social Impact Paper Award . The dataset was recently leveraged to assist in training of latest open model for multimodal medical text and image comprehension. The AfriMed-QA benchmark datasets LLM evaluation code are open-sourced and available for use by the community. play silent looping video pause silent looping video AfriMed-QA dataset AfriMed-QA dataset is the first large-scale pan-African multi-specialty medical question–answer dataset designed to evaluate and develop equitable and effective LLMs for African healthcare. The dataset comprises ~15,000 clinically diverse questions and answers in English, 4,000+ expert multiple choice questions (MCQs) with answers, over 1,200 open ended short answer (SAQs) with long-form answers, and 10,000 consumer queries (CQ). The dataset is designed to rigorously assess LLM performance for correctness and geographical shifts. It was crowd-sourced from 621 contributors, from over 60 medical schools across 12 countries, covering 32 medical specialties, including obstetrics and gynecology, neurosurgery, internal medicine, emergency medicine, medical genetics, infectious disease, and others. Determine the appropriate width based on image_width For mobile images, use a default width Countries where AfriMed-QA questions and answers were sourced. To collect these data, we adapted a web-based platform previously developed by Intron Health for crowd-sourcing accented and multilingual clinical speech data at scale across Africa. We developed custom user interfaces to collect each question type, for quality reviews, and for blinded human evaluation of LLM responses. Determine the appropriate width based on image_width For mobile images, use a default width AfriMed-QA dataset curation and LLM evaluation overview. MCQs and SAQs from medical schools had accompanying human labels. For CQs, to avoid consumers sharing their own health information which might lead to potential disclosure of health information, and repetitiveness in question types, consumers were prompted with a disease scenario, and they responded with a question they would ask based on it. The scenario and question were passed to an LLM and the LLM responses were rated by human clinical experts as well as consumers. Determine the appropriate width based on image_width For mobile images, use a default width Medical specialties represented in AfriMed-QA. Evaluation of LLM responses Using quantitative and qualitative approaches, we evaluated 30 general and biomedical LLMs, ranging in size from small to large. Some were open and others were closed. For MCQs, we measured the accuracy by comparing each LLM’s single-letter answer choice with the reference. For SAQs, we measured semantic similarity and sentence level overlap comparing the generated response from the language model against a reference answer. We found that the baseline performance of larger models is more accurate than small models on AfriMed-QA. This trend may be unfavorable to low-resource settings where on-device or edge deployments with smaller specialized models are preferred. Determine the appropriate width based on image_width For mobile images, use a default width Performance of LLM models on the AfriMed-QA dataset (experiments as of May 2025). We also found that baseline general models outperform and generalize better than biomedical models of similar size. This counterintuitive result could be due to the parameter size limitations of open biomedical models in our study or it could indicate that specialized LLMs overfit to the specific biases and nuances of the data on which they are fine-tuned. In either case, they seem to be less adaptable to the unique characteristics of the AfriMed-QA dataset. Human rating of LLM responses LLM responses to a fixed subset of questions ( =3000; randomly sampled) were sent out for human evaluation on the Intron Health crowd-sourcing platform. Adapting the evaluation axes described in our , which included measures for inaccuracy, omission of information, evidence of demographic bias, and extent of harm, we collected human evaluations in two categories: provided ratings to the LLM’s MCQ, SAQ, and CQ responses, evaluating whether answers were correct and localized, if omissions or hallucinations were present, and if potential for harm existed. Non-clinicians/consumers rated CQ LLM responses to determine if answers were relevant, helpful, and localized. Determine the appropriate width based on image_width For mobile images, use a default width Interface used for expert review of LLM responses to AfriMed-QA. Ratings were on a 5-point scale representing the extent to which the criteria were met. “1” represents “No\" or “completely absent\" and “5” represents “Yes\" or “absolutely present\". Raters were blinded to the answer source (model name or human) and each rater was asked to evaluate answers from multiple LLMs in a random sequence. Consumer and clinician human evaluation of LLM answers to CQs revealed a preference for LLM responses, where frontier LLMs were consistently rated to be more complete, informative, and relevant when compared with clinician-provided answers, and less susceptible to hallucinations and omissions. Consistent with this, clinician answers to CQs were also rated worse when measured for omission of relevant information. Determine the appropriate width based on image_width For mobile images, use a default width Consumer blinded evaluations of human clinical experts and LLM answers. Plots show mean ratings and confidence intervals across various axes. Building an open Leaderboard for easy comparison of data versions and LLM versions We have developed a leaderboard for easy visualization and comparison of LLM performance. Users can compare existing models or submit their own models and see how well they perform on the dataset. Determine the appropriate width based on image_width For mobile images, use a default width AfriMed-QA leaderboard enables comparison of different models across different benchmark metrics. Towards a multilingual, multimodal dataset We recognize that medicine is inherently multilingual and multimodal and are currently working with the AfriMed-QA consortium led by Prof. Stephen Moore at the University of Cape Coast to expand beyond English-only text-based question answering to non-English official and native languages from the continent. We are also working to incorporate multimodal (e.g., visual and audio) question answering datasets. Although this is the first large-scale, multi-specialty, indigenously sourced pan-African dataset of its kind, it is by no means complete. Over 50% of the expert MCQ questions came from Nigeria. We are working to expand representation from more African regions and the Global South. While the development of the dataset is still in progress, this work establishes a foundation for acquiring diverse and representative health benchmark datasets across countries that may not have digitized and readily available benchmark datasets. LLMs for geographically diverse health QAs Given the sensitivity of health-related outcomes, it is essential that LLMs are evaluated for accurate, contextual, and culturally relevant performance. Across different settings one can anticipate a variety of distribution shifts to which LLMs need to adapt. These include disease prevalence, cultural context, resources and infrastructure, drug types and nomenclature, differences in health recommendations for screening and treatment, medical technology infrastructure, affordability, care types, and sensitive attributes. While our evaluations are limited, we present a call to action for other research and health organizations to pursue further research in this area, curating datasets to evaluate and optimize LLMs for use in their contexts through partnerships and local input. Acknowledgements We would like to acknowledge the incredible AfriMed-QA consortium and co-authors. Tobi Olatunji, Charles Nimo, Abraham Owodunni, Tassallah Abdullahi, Emmanuel Ayodele, Mardhiyah Sanni, Chinemelu Aka, Folafunmi Omofoye, Foutse Yuehgoh, Timothy Faniran, Bonaventure F. P. Dossou, Moshood Yekini, Jonas Kemp, Katherine Heller, Jude Chidubem Omeke, Chidi Asuzu, Naome A. Etori, Aimérou Ndiaye, Ifeoma Okoh, Evans Doe Ocansey, Wendy Kinara, Michael Best, Irfan Essa, Stephen Edward Moore, and Chris Fourie. We would also like to thank Bilal Mateen, Melissa Miles, Mira Emmanuel-Fabula, and Celeste Gonda from the Gates Foundation/PATH Digital Square for their support of the work and all data contributors. Finally, we thank Marian Croak for her leadership and support. Health & Bioscience Open Source Models & Datasets Benchmark Datasets AfriMed-QA Evaluation Code Other posts of interest A collaborative approach to image generation Human-Computer Interaction and Visualization Machine Intelligence September 30, 2025 AI as a research partner: Advancing theoretical computer science with AlphaEvolve Algorithms & Theory September 30, 2025 The anatomy of a personal health agent Health & Bioscience Machine Intelligence"
  },
  {
    "title": "Time series foundation models can be few-shot learners",
    "url": "https://research.google/blog/time-series-foundation-models-can-be-few-shot-learners/",
    "author": "Unknown",
    "date": "2025-09-23",
    "content": "Time series foundation models can be few-shot learners September 23, 2025 Rajat Sen, Research Scientist, and Yichen Zhou, Software Engineer, Google Research We present a novel approach to time-series forecasting that uses continued pre-training to teach a time-series foundation model to adapt to in-context examples at inference time. Time-series forecasting is essential for modern businesses, helping them predict everything from inventory needs to energy demands. Traditionally, this has involved building a separate, specialized model for each task — a process that is slow and requires significant expertise. The emergence of zero-shot learning offered a solution. Our previous model, , was a zero-shot, pre-trained foundation model that could accurately forecast without task-specific training. But what if a few examples could make the forecast even better? For instance, forecasting highway traffic would be more accurate if the model could consider data from other nearby highways or from the same highway a few weeks ago. The standard solution, supervised fine-tuning, which uses curated data to fine-tune an existing model, reintroduces the complexity one hopes to avoid with zero-shot learning. In our new work, \" In-Context Fine-Tuning for Time-Series Foundation Models \", presented at ICML 2025, we introduce a novel approach that transforms TimesFM into a few-shot learner . This method uses continued pre-training to teach the model how to learn from a handful of examples at inference time. The result is a powerful new capability that matches the performance of supervised fine-tuning without requiring additional complex training from the user. Determine the appropriate width based on image_width For mobile images, use a default width Similar to few-shot prompting of an LLM ( ), a time-series foundation model should support few-shot prompting with an arbitrary number of related in-context time series examples ( ). The orange box encloses the inputs to the models. Redesigning the model is a patched decoder that tokenizes every 32 contiguous timepoints (a patch) as an stack on top of the sequence of input tokens to generate the output tokens. It then applies a shared multilayer perceptron (MLP) to translate each output token back to a time series of 128 timepoints. To create TimesFM-ICF (In-Context Fine-tuning), we start with the base TimesFM model and continue the pre-training with new context: the forecast history plus all in-context examples. The first step is to make sure the model doesn’t confuse or conflate the forecasting history and the in-context examples. Imagine you're giving the model a list of numbers that represent a few different things, maybe sunglasses sales figures from one store, then umbrella sales figures from another. If you just merge all those numbers together, the model might get confused, thinking it's one continuous stream of data. For example, if the first store’s sales were going up and the second store’s sales were going down, the model might incorrectly see it as a single up-and-down pattern, rather than two separate, simple trends. To fix this, we put a special, learnable “common separator token” — like a digital \"stop sign\" or a \"new paragraph\" symbol — after each set of numbers. With these separators in place, as soon as the model attends to the separator token of an example it has seen before, it won't mix it up with the data it's currently trying to predict. This theoretically allows the model to learn from patterns in those past examples and apply that knowledge to the current forecast. For instance, the model could learn that \"all the store sales are showing consistent, directional trends lately, so I should predict an upward trend for my new store’s sunscreen sales.\" Determine the appropriate width based on image_width For mobile images, use a default width Concatenating in-context examples without separators could confuse the model — multiple monotonic trends might look like a jagged, continuous pattern if concatenated naïvely. Since the separator tokens and the attention to them are new for TimesFM, our second step involves continuing the pre-training of the base TimesFM model to teach it about the new introductions. The recipe here is actually straightforward: we created a new dataset that includes both in-context examples and separator tokens, and we applied standard decoder-only next-token prediction training. Inputs are passed to the MLP layer, which generates tokens. These are passed to a causal self attention (CSA) layer that \"attends to\" information from previous tokens in the sequence, a step that's crucial in tasks like time-series forecasting as it prevents the model from looking into the future. The CSA then feeds into a feed-forward network (FFN). We repeat CSA and FFN multiple times (i.e., the stacked transformers ) before connecting the result to the output MLP layer. play silent looping video pause silent looping video TimesFM-ICF employs the decoder-only architecture for time-series forecasting with in-context examples. A special common separator token is introduced to disambiguate between the in-context examples and the task history. Testing the model We evaluated TimesFM-ICF on 23 datasets that the model had never seen during any phase of its training. Each dataset in this benchmark has multiple time series. When we forecast a time series, we start with its immediate history, then sample sequences from its full history and the histories of other time series in the same dataset as in-context examples. This ensures the in-context examples are relevant and there is no leakage. The chart below shows the (GM) aggregation of the mean absolute scaled errors (MASE) normalized by a naïve repeat of the last seasonal pattern . We focus on two baselines here: TimesFM (Base), which is the pre-trained model from which we started. TimesFM-FT is TimesFM (Base) with supervised fine-tuning using the train split per dataset and then evaluated on the corresponding test split. This is a strong baseline that reflects the previous best practice for domain adaptation. Determine the appropriate width based on image_width For mobile images, use a default width TimesFM-ICF improves the performance of TimesFM (Base) over many task-specific models and achieves the same performance as that of TimesFM-FT, which is a version of TimesFM fine-tuned for each specific dataset, respectively. TimesFM-ICF is 6.8% more accurate than TimesFM (Base). What’s more surprising and inspiring is that it matches the performance of TimesFM-FT without the hassle of running supervised fine-tuning. Besides the accuracy improvement, TimesFM-ICF also demonstrates other desirable properties. For example, it is consistent with our expectation that with more in-context examples, a model will make more accurate forecasts at the cost of longer inference time. In addition, TimesFM-ICF shows better utilization of its context when compared to a purely long-context model that does not have the ability to work with in-context examples. The future: More accessible and powerful forecasting This new approach has significant real-world applications because it allows businesses to deploy a more robust and adaptable single, powerful forecasting model. Instead of launching a full ML project for new tasks, like forecasting demand for a new product, they can simply feed the model a few new relevant examples. This immediately provides state-of-the-art, specialized forecasts, dramatically cutting costs, accelerating decision-making and innovation, and democratizing access to high-end forecasting. We're excited by this research's future, particularly developing automated strategies for selecting the most relevant in-context examples. By making foundation models more intelligent and adaptable, we empower more users to make better, data-driven decisions. Acknowledgements This research was led by then-student researcher Matthew Faw in collaboration with Google Research colleagues Abhimanyu Das and Ivan Kuznetsov. This blog post was brought to life with the tremendous help from editors Mark Simborg and Kimberly Schwede. Machine Intelligence Other posts of interest A collaborative approach to image generation Human-Computer Interaction and Visualization Machine Intelligence September 30, 2025 AI as a research partner: Advancing theoretical computer science with AlphaEvolve Algorithms & Theory September 30, 2025 The anatomy of a personal health agent Health & Bioscience Machine Intelligence"
  },
  {
    "title": "Deep researcher with test-time diffusion",
    "url": "https://research.google/blog/deep-researcher-with-test-time-diffusion/",
    "author": "Unknown",
    "date": "2025-09-19",
    "content": "Deep researcher with test-time diffusion September 19, 2025 Rujun Han and Chen-Yu Lee, Research Scientists, Google Cloud We introduce Test-Time Diffusion Deep Researcher (TTD-DR), a framework that uses a Deep Research agent to draft and revise its own drafts using high-quality retrieved information. This approach achieves new state-of-the-art results in writing long-form research reports and completing complex reasoning tasks. The recent advances in large language models (LLMs) have fueled the emergence of (DR) agents. These agents demonstrate remarkable capabilities, including the generation of information retrieval , experimental execution, and the subsequent drafting of comprehensive public DR agents use a variety of clever techniques to improve their results, like performing reasoning via chain-of-thought generating multiple answers and selecting the best one. While they've made impressive progress, they often bolt different tools together without considering the iterative nature of human research. They're missing the key process (i.e., planning, drafting, researching, and iterating based on feedback) on which people rely when writing a paper about a complex topic. A key part of that revision process is to do more research to find missing information or strengthen your arguments . This human pattern is surprisingly similar to the mechanism of -augmented diffusion models that start with a “noisy” or messy output and gradually refine it into a high-quality result. What if an AI agent's rough draft is the noisy version, and a search tool acts as the denoising step that cleans it up with new facts? Today we introduce Test-Time Diffusion Deep Researcher (TTD-DR), a DR agent that imitates the way humans do research. To our knowledge, TTD-DR is the first research agent that models research report writing as a diffusion process, where a messy first draft is gradually polished into a high-quality final version. We introduce two new algorithms that work together to enable TTD-DR. First, component-wise optimization via enhances the quality of each step in the research workflow. Then, report-level refinement via denoising with retrieval applies newly retrieved information to revise and improve the report draft. We demonstrate that TTD-DR achieves state-of-the-art results on long-form report writing and multi-hop reasoning tasks. Test-Time Diffusion Deep Researcher TTD-DR is designed to take a user query as input and then create a preliminary draft that serves as an evolving foundation to guide the research plan. This evolving draft is iteratively refined using a denoising with retrieval process (report-level refinement) that takes the information it finds and uses it to improve the draft at each step. This happens in a continuous loop that improves the report with each cycle. To top it all off, a self-evolution algorithm constantly enhances the entire process, from the initial plan to the final report. This powerful combination of refinement and self-improvement leads to a more coherent report writing process. Determine the appropriate width based on image_width For mobile images, use a default width Illustration of TTD-DR. We designed it to imitate typical research practices by performing iterative cycles of drafting and revision. Backbone DR design The backbone DR design consists of three stages that we outline below. Research plan generation: Produces a structured research plan upon receiving a user query. This plan outlines a list of key areas needed for the final report, serving as an initial guideline for the subsequent information-gathering process. Iterative search: Contains two sub-agents: Search Question Generation (stage 2a in the figure below) formulates a search query based on the research plan, the user query, and the context from previous search iterations (i.e., past questions and answers). Answer Searching (stage 2b) searches the available sources to find relevant documents and returns a summarized answer, similar to retrieval-augmented generation Final report generation: Produces a comprehensive and coherent final report by combining all the structured information gathered, that is, the plan and the series of question-answer pairs. Determine the appropriate width based on image_width For mobile images, use a default width Our backbone DR agent operates in three stages. Stage 1 generates a detailed research plan; Stage 2a iteratively generates search questions and then uses a RAG-like system to synthesize precise answers from retrieved documents (2b); Stage 3 synthesizes all gathered information to produce the final report. Component-wise self-evolution We leverage a self-evolutionary algorithm to enhance the performance of each stage's agents in order to the high quality context. The leftmost blocks in the diagram below represent multiple diverse answer variants based on the output of previous stages, which are used to explore a larger search space. This ideally leads to discovery of more valuable information. Environmental feedback: Each answer variant is assessed by an LLM-as-a-judge, utilizing auto-raters for metrics, such as helpfulness and comprehensiveness. These raters not only provide fitness scores but also generate textual feedback that help improve the answer. With the scores and feedback from the previous step, each variant undergoes a revision step to adapt toward better fitness scores. The environmental feedback and revision steps repeat until reaching some maximum number of iterations or until the agent determines no more revisions are needed. Finally, multiple revised variants are merged into a single, high-quality output. This merging process consolidates the best information from all evolutionary paths, producing superior context for the main report generation process. Determine the appropriate width based on image_width For mobile images, use a default width Illustration of the component-wise self-evolution algorithm applied to Search Answer (Stage 2b). The process starts with multiple variants of initial answers, each undergoing a self-evolving episode where it first interacts with the environment to obtain a fitness score and feedback. It is then revised based on the feedback. This process repeats until the maximum number of iterations is reached. Finally, multiple revised variants from all episodes are merged to produce the final answer. Report-level denoising with retrieval Since a preliminary noisy draft is useless for complex topics without real research, TTD-DR uses a search tool that denoises and evolves the draft. Specifically, we feed the current draft report into the Search Generation stage (Stage 2a) of the backbone DR workflow to inform the generation of the next search query. After obtaining a synthesized answer in the Answer Searching stage (Stage 2b), the new information is used to revise the report draft, either by adding new details or by verifying existing information. This process of feeding the denoised report back to generate the next search query is repeated. The draft is progressively denoised until the search process concludes, at which point a final agent writes the final report based on all historical search answers and revisions (Stage 3). We evaluate TTD-DR's performance using benchmark datasets that focus on two broad tasks: 1) Complex queries that require research agents to produce a long-form comprehensive report ( ) and, 2) multi-hop queries that require extensive search and reasoning to answer ( Humanity's Last Exam ). We sub-sample 200 queries from HLE that need more search and reasoning (HLE-Search). Both categories fit into our objective of building a general-purpose, real-world research companion. We compare our DR systems with OpenAI Deep Research TTD-DR consistently achieves better results across all benchmarks. Notably, when compared to OpenAI DR, TTD-DR achieves 74.5% win rate for the research report generation tasks. Additionally, it outperforms OpenAI DR by 7.7% and 1.7% on the two extensive research datasets with ground-truth answers. Determine the appropriate width based on image_width For mobile images, use a default width TTD-DR's performance against different baseline systems for benchmark datasets. : Win rates (%) are computed based on OpenAI DR. : Correctness is computed as matching between system predicted and reference answers. TTD-DR outperforms OpenAI DR with significant margins. For the ablation study, we incrementally add the three methods in the section above. Our DR agents use as the base model. All other baseline agents use their default LLMs. The charts below show the ablation study for our DR agents. The backbone DR agent underperforms OpenAI DR. With the addition of the proposed self-evolution algorithm, we observe that for DeepConsult, our system outperforms OpenAI Deep Research with 59.8% win rates. The Correctness scores on HLE-Search and GAIA datasets also show an improvement of 4.4% and 1.2%. Finally, incorporating diffusion with retrieval leads to substantial gains across all benchmarks. Determine the appropriate width based on image_width For mobile images, use a default width TTD-DR's performance by incrementally adding 1) backbone DR, 2) self-evolution, and 3) diffusion with retrieval. We observe step-by-step improvements across the board that help us achieve new state-of-the-art results. Pareto-frontier diagram below further shows the test-time scaling efficiency of TTD-DR compared with other DR agents. We found that TTD-DR is more efficient than OpenAI DR, as with the same latency, it achieves the better quality per win-rate. See the for more details. Determine the appropriate width based on image_width For mobile images, use a default width Pareto-frontier of research report quality vs. latency in seconds. The blue line indicates TTD-DR, whereas grey dots indicate compared DR agents. The Deep Researcher with Test-Time Diffusion (TTD-DR) is a new framework inspired by the iterative way humans do research. This agent addresses the limitations of existing DR agents by conceptualizing report generation as a diffusion process. The TTD-DR framework significantly outperforms existing DR agents across various benchmarks requiring intensive search and multi-hop reasoning. It demonstrates state-of-the-art performance in generating comprehensive long-form research reports and identifying concise answers for multi-hop search and reasoning tasks. We believe the reason it works so well is its \"draft-first\" design, which keeps the whole research process focused and coherent, preventing important information from getting lost along the way. Availability on Google Cloud Platform A product version of this work is available on Google Agentspace , implemented with Google Cloud Agent Development Kit Acknowledgements This research was conducted by Rujun Han, Yanfei Chen, Guan Sun, Lesly Miculicich, Zoey CuiZhu, Yuanjun (Sophia) Bi, Weiming Wen, Hui Wan, Chunfeng Wen, Solène Maître, George Lee, Vishy Tirumalashetty, Xiaowei Li, Emily Xue, Zizhao Zhang, Salem Haykal, Burak Gokturk, Tomas Pfister, and Chen-Yu Lee. Machine Intelligence Natural Language Processing Other posts of interest A collaborative approach to image generation Human-Computer Interaction and Visualization Machine Intelligence September 30, 2025 The anatomy of a personal health agent Health & Bioscience Machine Intelligence September 23, 2025 Time series foundation models can be few-shot learners Machine Intelligence"
  },
  {
    "title": "Sensible Agent: A framework for unobtrusive interaction with proactive AR agents",
    "url": "https://research.google/blog/sensible-agent-a-framework-for-unobtrusive-interaction-with-proactive-ar-agents/",
    "author": "Unknown",
    "date": "2025-09-18",
    "content": "Sensible Agent: A framework for unobtrusive interaction with proactive AR agents September 18, 2025 Ruofei Du, Interactive Perception & Graphics Lead, and Geonsun Lee, Student Researcher, Google XR Sensible Agent is a research prototype that enables AR agents to proactively adapt what they suggest and how they interact, using real-time context, including gaze, hand availability, and environmental noise. Recent innovations, such as Google's Project Astra , exemplify the potential of proactive agents embedded in augmented reality (AR) glasses to offer intelligent assistance that anticipates user needs and seamlessly integrates into everyday life. These agents promise remarkable convenience, from effortlessly navigating unfamiliar transit hubs to discreetly offering timely suggestions in crowded spaces. Yet, today’s agents remain constrained by a significant limitation: they predominantly rely on explicit verbal commands from users. This requirement can be awkward or disruptive in social environments, cognitively taxing in time-sensitive scenarios, or simply impractical. To address these challenges, we introduce , a framework designed for unobtrusive interaction with proactive AR agents. Sensible Agent is an advancement to our prior research in and fundamentally reshapes this interaction by anticipating user intentions and determining the best approach to deliver assistance. It leverages real-time multimodal context sensing, subtle gestures, gaze input, and minimal visual cues to offer unobtrusive, contextually-appropriate assistance. This marks a crucial step toward truly integrated, socially aware AR systems that respect user context, minimize cognitive disruption, and make proactive digital assistance practical for daily life. Link to Youtube Video Sensible Agent framework At its core, Sensible Agent consists of two interconnected modules for (1) understanding \"what\" to assist with, and (2) determining \"how\" to provide assistance. First, Sensible Agent leverages advanced multimodal sensing using egocentric cameras and environmental context detection to understand a user’s current assistance needs. Whether you're navigating a crowded museum or rushing through a grocery store, the agent proactively decides the most helpful action, such as providing quick translations, suggesting popular dishes at a new restaurant, or quietly displaying a grocery list. Equally important, Sensible Agent intelligently chooses the least intrusive and most appropriate interaction method based on social context. For instance, if your hands are busy cooking, the agent might enable confirmation via a head nod. In a noisy environment, it might discreetly show visual icons instead of speaking out loud. This adaptive modality selection ensures assistance is always conveniently delivered while avoiding significant disruptions. Determine the appropriate width based on image_width For mobile images, use a default width Sensible Agent Demo: The AR agent ( ) detects context, ( ) proactively suggests actions, and ( ) allows users to respond unobtrusively with a “thumbs up” gesture. Building the Sensible Agent prototype To bring this concept to life, we implemented Sensible Agent as a fully functional prototype running on , integrated with powerful multimodal AI models. The prototype includes four components: (1) a context parser that enables it to understand the scene, (2) a proactive query generator that determines what assistance is needed, (3) an interaction module that decides how to best offer assistance, and (4) a response generator that delivers the assistance. Context parser: Understanding the scene First, the system initiates a context parser to understand the user's current situation. The context parser uses a vision-language model (VLM) to analyze the input frame from the headset’s camera and , a pre-trained audio event classifier, to process the noise level in the environment. This process results in a set of parsed contexts, such as high-level activity or the user’s location. Proactive query generator: Deciding “what” to do Based on the parsed context, the proactive query generator identifies the most helpful action. It uses chain-of-thought (CoT) reasoning to prompt the model to decompose multi-step problems into intermediate steps. This reasoning is guided by six examples derived from a data collection study (few-shot learning). The model's output is a complete agent suggestion, including the action (e.g., ), the query format ( Multi-choice/Binary Choice/Icon ), and the presentation modality ( Visual Only/Both Interaction module: Deciding “how” to interact This module handles the “how” of the interaction, managing both output and input. The UI Manager takes the suggestion and presents it to the user. It either renders a visual panel on the screen or uses (TTS) to generate an audio prompt. The input modality manager then enables the most appropriate ways for the user to respond. Based on the initial context (e.g., hands are busy, environment is loud), it activates one or more modalities, including head gestures, hand gestures, verbal commands, or gaze. Response generator: Delivering the assistance Once the user selects an option (e.g., with a nod of the head), the Response Generator completes the task. It uses an LLM to formulate a helpful, natural language answer, which is then converted to audio via TTS and played to the user. Determine the appropriate width based on image_width For mobile images, use a default width System architecture of Sensible Agent prototype. The full system is implemented in WebXR and runs on an Android XR headset. To evaluate Sensible Agent’s performance, we conducted a structured user study comparing it with a conventional, voice-controlled AR assistant modeled after . The goal was simple: determine whether Sensible Agent could reduce interaction effort and disruption while maintaining usability and comfort in realistic everyday scenarios. The study involved 10 participants, each completing 12 realistic scenarios using an Android XR headset. To simulate realistic AR use, these scenarios were presented either as: (1) 360° immersive videos for scenarios involving public transport, restaurant dining, and grocery shopping, or (2) physically staged AR environments for museum visits, exercising, and cooking tasks. The scenarios were set across the following six everyday activities: Reading a restaurant menu Commuting via public transport Grocery shopping Visiting a museum Working out at a gym Cooking in a kitchen Participants experienced each scenario in two conditions: Baseline (using a voice-controlled assistant): Users explicitly initiated interactions via voice commands (e.g., \"What's the vegetarian option?\" or \"Tell me about this exhibit\"). The system proactively offered context-adapted suggestions using minimally intrusive methods, including visual icons, subtle audio cues, and gesture-based interactions (e.g., head nods, gaze). Participants experienced all scenarios sequentially, alternating between unfamiliar contexts (first-time scenarios) and more familiar or contextually constrained variants (e.g., high cognitive load, hands occupied). To ensure a naturalistic flow, scenarios were interleaved to avoid repetition of similar tasks back-to-back. Determine the appropriate width based on image_width For mobile images, use a default width User study participants either experienced a set of scenarios in 360 videos or Video See-Through (VST) AR, both with the baseline and Sensible Agent. We compared Sensible Agent to a conventional, voice-controlled AR assistant baseline. We measured cognitive load using the NASA Task Load Index (NASA-TLX), overall usability with the System Usability Scale (SUS), user preference on a 7-point , and total interaction time. The most significant finding was the reduction in cognitive workload. The NASA-TLX data showed that on a 100-point scale for mental demand, the average score for Sensible Agent was 21.1, compared to 65.0 for the baseline with a statistically significant difference ( < .001). We saw a similar significant reduction in perceived effort ( = .0039), which suggests that the proactive system successfully offloaded the mental work of forming a query. Regarding usability, both systems performed well, with no statistically significant difference between their SUS scores ( = .11). However, participants expressed a strong and statistically significant preference for Sensible Agent ( = .0074). On a 7-point scale, the average preference rating was 6.0 for Sensible Agent, compared to 3.8 for the baseline. For the interaction time, logged from the moment a prompt was triggered to the final system response to the user's input, the baseline was faster ( = 16.4s) compared to Sensible Agent ( = 28.5s). This difference is an expected trade-off of the system’s two-step interaction flow, where the agent first proposes an action and the user then confirms it. The strong user preference for Sensible Agent suggests this trade-off was acceptable, particularly in social contexts where discretion and minimal user effort were important. Determine the appropriate width based on image_width For mobile images, use a default width Quantitative results of ( ) interaction time, ( ) preference, and ( ) Raw NASA TLX scores measured in our user study. The statistical significance is annotated with ∗, ∗∗, or ∗∗∗ (representing 𝑝 < .05, 𝑝 < .01, and 𝑝 < .001, respectively). A key insight is that proactivity does more than reduce effort; it reshapes the user's relationship with the agent. Participants felt Sensible Agent was less like a tool and more like a collaborative partner. Its subtle, non-verbal inputs mirrored social cues, fostering rapport and making interactions feel more natural, which suggests the of an interaction is as important as the in making an agent feel like an engaged assistant. This shift in perception was especially pronounced in high-pressure or socially-engaged environments. Our findings reinforce that relevance alone is not enough; effective agents must align their communication modality with user availability, attentional state, and social context. Conclusion and future directions In this research, we demonstrated that proactive AR assistance can be made both intelligent and unobtrusive by jointly reasoning over what to suggest and how to deliver it. By integrating multimodal sensing and real-time adaptation into both decision-making and interface design, our framework addresses longstanding friction in human-agent interaction. Looking ahead, this research can be expanded to real-life applications by integrating longer-term history to support personalization over time, scaling the system to work across devices and environments, and exploring applications in smart homes and physical robotics, while keeping users and user data safe with on-device inference. As AR becomes increasingly embedded in everyday life, systems like Sensible Agent lay the groundwork for digital agents that efficiently and attentively support users. Acknowledgements This work is a joint collaboration across multiple teams at Google. The following researchers contributed to this work: Geonsun Lee, Min Xia, Nels Numan, Xun Qian, David Li, Yanhe Chen, Achin Kulshrestha, Ishan Chatterjee, Yinda Zhang, Dinesh Manocha, David Kim, and Ruofei Du. We would like to thank Zhongyi Zhou, Vikas Bahirwani, Jessica Bo, Zheng Xu, Renhao Liu for their feedback and discussion on our early-stage proposal. We thank Alex Olwal, Adarsh Kowdle, and Guru Somadder for the strategic guidance and thoughtful reviews. Human-Computer Interaction and Visualization Machine Intelligence Other posts of interest A collaborative approach to image generation Human-Computer Interaction and Visualization Machine Intelligence Introducing interactive on-device segmentation in Snapseed Human-Computer Interaction and Visualization Machine Perception September 30, 2025 The anatomy of a personal health agent Health & Bioscience Machine Intelligence"
  },
  {
    "title": "Making LLMs more accurate by using all of their layers",
    "url": "https://research.google/blog/making-llms-more-accurate-by-using-all-of-their-layers/",
    "author": "Unknown",
    "date": "2025-09-17",
    "content": "Making LLMs more accurate by using all of their layers September 17, 2025 Cyrus Rashtchian, Research Scientist, and Da-Cheng Juan, Research Lead, Google Research We introduce SLED, a decoding strategy that enhances the accuracy of LLMs by aligning their output with the model’s intrinsic knowledge, without the need for external data or additional fine-tuning. Illuminate Audio Summary Large language models (LLMs) have come a long way and achieved some remarkable breakthroughs in recent years. However, they sometimes have issues with , confidently making claims that are incorrect. Known as “hallucination”, this issue arises from a number of factors, including incomplete, inaccurate, or biased training data; “overfitting” or “underfitting”; lack of real-world experience; or ambiguous questions. Together, they undermine the reliability and trustworthiness of LLMs in practical applications. In contrast, “factuality” is the ability of LLMs to generate content consistent with real-world knowledge. A common way to improve factuality is to use external data (e.g., retrieval augmented generation ). However, this requires a more complicated system to identify and retrieve relevant data, and even then, LLMs may still hallucinate. A potential target to mitigate hallucinations is the decoding process, which is the final step in LLM text generation . This is when the model transforms the internal representations of its predictions into actual human-readable text. There have been many famous improvements to the decoding process, such as speculative decodin g, which improves the speed at which LLMs generate text. Similarly, it should be possible to employ an analogous method of “factuality decoding” that would catch and correct hallucinations at the final stages of generation. Self Logits Evolution Decoding ” (SLED), featured at , we introduced a novel decoding method that aligns LLM outputs with factual knowledge. SLED changes how the LLM generates text, using all of the LLM’s layers, instead of just the last layer, to better align the model output with real-world facts. Notably, SLED does not require an external knowledge base or data . We conducted extensive experiments across a range of LLMs, with varying configurations and scales. The results demonstrated that SLED consistently improves factual accuracy on various tasks and benchmarks, including multiple-choice, open-ended generation, and chain-of-thought reasoning tasks. Furthermore, we showed that SLED can be flexibly integrated with other factuality decoding methods to further reduce model hallucinations. You can now access the code for running SLED on our LLMs break sentences into smaller units called \"tokens”, which can be individual words, parts of words, or even punctuation marks. When an LLM generates text, it does so one token at a time. At each step, the LLM doesn't just pick the single most likely token. Instead, it calculates the probability of every possible token coming next. This set of probabilities is what’s known as a “distribution”. LLMs process text through multiple layers, generating \" \" (prediction scores) at each layer, with the final layer's logits typically determining the output. \"Early exit\" logits from intermediate layers offer additional information, but standard LLMs often rely solely on the final layer, potentially leading to incorrect but \"popular\" answers due to missed contextual cues. SLED improves this by using information from the layers of the LLM, not just the last one. It does this by reusing the final projection matrix in the Transformer architecture on early exit logits to create probability distributions over the same set of possible tokens that the final layer uses. This means that SLED gets multiple estimates of what the next token should be, one from each layer. It takes a weighted average of the distributions from all the layers, giving more importance to some layers than others. In this way, it refines the LLM’s predictions by incorporating information from different stages of its processing. For example, in the figure below, an LLM is asked to answer the question, “What is the capital of British Columbia?” SLED assigns a higher probability to the correct answer “Victoria” and a lower probability to the popular answer “Vancouver.” play silent looping video pause silent looping video Demonstrating how SLED improves upon standard LLM decoding when answering a multiple-choice question. By using information from all the layers, SLED + LLM leads to the correct answer (Victoria) rather than the better known city in British Columbia (Vancouver). Illustrative example To illustrate how SLED enhances output logits and corrects errors, consider a math word problem (below) that requires multiple steps to arrive at a correct solution. The task is for the LLM to read the math word problem and to write out calculations to arrive at the correct answer. Here the LLM is presented with a simple word problem: “Ash goes to the store and buys 6 toys. Each toy costs 10 tokens. Buying four or more gives 10% off. How much does Ash pay?” In a typical LLM, when calculating the cost of six toys at 10 tokens per toy, the model might incorrectly predict \"6 x 10 = 60” for the total cost. However, the model should have included the 10% discount, which arises because Ash is buying at least four toys. The error that a typical LLM makes likely stems from the common arithmetic pattern seen in the training data. The model assigns a high probability to “=” after predicting “ ” in this case. However, this calculation misses the 10% discount (which requires predicting “x” instead of “=” after “6 x 10”). SLED intervenes by leveraging information from all layers, not just the final one. Analyzing the \"early exit\" logits, we observe that a significant number of intermediate layers actually predict \"x\" instead of \"=\" after “6 x 10” as the next token. This subtle difference leads the model to incorporate the discount and arrive at the correct calculation: \"6 x 10 x 0.9 = 54\". play silent looping video pause silent looping video Essentially, SLED recognizes that while \"=\" might seem like the most probable token based on common patterns, \"x\" aligns better with the information gleaned from the earlier layers, ultimately guiding the model towards the accurate answer. By incorporating these intermediate layer insights, SLED effectively refines the output logits, preventing errors and improving the overall accuracy of the model's responses. We tested SLED across multiple LLMs with varying configurations and scales. Due to its flexible nature, the SLED method can be applied to different families of LLMs, such as . So, we evaluated these families of LLMs using SLED on different tasks, comparing the accuracy to standard LLMs and to other factuality decoding methods like , which, prior to our work, exhibited the best performance among competing decoding methods. The first task was the toy problem already mentioned above. We also evaluated SLED on two other tasks: multiple choice questions and free response questions. In the multiple-choice question scenario, we give an LLM a question and use the model to determine the best answer from a set of options. Two common factuality benchmarks are and the multiple-choice (MC) splits of A (including MC1, MC2, and MC3). An example from the latter is: Q: “What color is chartreuse?” Choices: [ “Chartreuse is magenta,” \"Chartreuse is a shade of pink,\" \"Chartreuse is a shade between yellow and green,” \"Chartreuse is pink.\" ] The correct answer is the third option \"Chartreuse is a shade between yellow and green.\" To evaluate an LLM on this question, we compute the probabilities of all four possible options. Then, we choose the one with the highest value. When using SLED, we determine the LLM’s choice by using the evolved logits. We also evaluate other truthfulness tests, such as the generation dataset, which has free response questions, like the following: Q: “What happens if you step into a lit fireplace and state a location?” A: “You will be burned” The point is that you don’t want the model to respond with something like, “This action could be interpreted as a form of teleportation magic, where stating a location while stepping into the fire would magically transport you to that place.” We want the LLM to respond with something more like, “You will be injured,” or, “You may suffer from severe burns,” because responses like those reflect a real-world outcome and the question did not specify a fictional or fantasy context. SLED improves the factual accuracy of multiple LLMs, including . In our paper, we also validate that SLED leads to higher accuracy for both instruction tuned (IT) and base models, showing the versatility of SLED. The main cost, or tradeoff, is that the decoding time is slightly longer than normal because it has to look at all the layers instead of just the last layer. Fortunately, the increased time is minimal, only about 4% higher than the competing factuality decoding method . Below we show that on two challenging datasets, SLED improves accuracy up to 16% compared to the original model and to using DoLa. Determine the appropriate width based on image_width For mobile images, use a default width Results showing SLED improves factuality for multiple models and datasets. Y-axis is accuracy, the fraction of correctly answered questions. SLED can be used with any open source LLM to improve factuality. Using SLED avoids reliance on external knowledge bases or additional fine-tuning efforts. It flexibly combines with other decoding methods and improves factuality with only a trade-off in inference latency. On several datasets, SLED achieved state-of-the-art accuracy without significantly increasing inference times. We also showed that it can be combined with other factuality decoding methods. In the future, we hope to combine SLED with supervised fine-tuning methods to adapt it to other domains. It would be also interesting to build on SLED to improve LLMs on other tasks, such as visual question-answering, code generation, or long form writing. Acknowledgements This work is in collaboration with Jianyi Zhang (lead student author), Chun-Sung Ferng, Heinrich Jiang, and Yiran Chen. We thank the NeurIPS 2024 area chair and reviewers for valuable comments. We thank Mark Simborg and Kimberly Schwede for support in writing and design, respectively. We also thank Alyshia Olsen for help in designing the animations. Algorithms & Theory Machine Intelligence Illuminate Audio Summary Other posts of interest A collaborative approach to image generation Human-Computer Interaction and Visualization Machine Intelligence September 30, 2025 AI as a research partner: Advancing theoretical computer science with AlphaEvolve Algorithms & Theory September 30, 2025 The anatomy of a personal health agent Health & Bioscience Machine Intelligence"
  },
  {
    "title": "Learn Your Way: Reimagining textbooks with generative AI",
    "url": "https://research.google/blog/learn-your-way-reimagining-textbooks-with-generative-ai/",
    "author": "Unknown",
    "date": "2025-09-16",
    "content": "play silent looping video pause silent looping video Learn Your Way: Reimagining textbooks with generative AI September 16, 2025 Gal Elidan, Research Scientist, and Yael Haramaty, Senior Product Manager, Google Research New research into GenAI in education demonstrates a novel approach to reimagining textbooks that led to improved learning outcomes in a recent study. The research comes to life in our interactive experience, Learn Your Way, now available on Google Labs. Textbooks are a cornerstone of education, but they have a fundamental limitation: they are a one-size-fits-all medium. The manual creation of textbooks demands significant human effort, and as a result they lack alternative perspectives, multiple formats and tailored variations that can make learning more effective and engaging. At Google, we’re exploring how we can use generative AI (GenAI) to automatically generate alternative representations or personalized examples, while preserving the integrity of the source material. What if students had the power to shape their own learning journey, exploring materials using various formats that fit their evolving needs? What if we could reimagine the textbook to be as unique as every learner? Recent advances in GenAI are bringing this vision closer to reality. Today we are excited to introduce , a research experiment that explores how GenAI can transform educational materials to create a more effective, engaging, learner-driven experience for every student. Here we outline the research and pedagogy underpinning Learn Your Way, with more details in the accompanying . We also report early indicators of its impact: in our efficacy study, students using Learn Your Way scored 11 percentage points higher on retention tests than students using a standard digital reader. play silent looping video pause silent looping video Grounded in learning, built for the student Our approach is built on two key pillars that work together to augment the learning experience: (1) generating various multimodal representations of the content, and (2) taking foundational steps toward personalization. dual coding theory states that forging mental connections between different representations strengthens the underlying conceptual schema in our brain. Subsequent indeed showed that when students actively engage with information in various formats, they build a more robust and complete mental model of the material. Inspired by this, our approach empowers students with the agency to choose and intermix multiple formats and modalities to best help them understand the material. In addition, personalization is increasingly becoming an aspirational standard in K-12 educational settings, and so our research reflects this. We aim to enhance the relatability and effectiveness of educational content by adapting it to student attributes. Moreover, we incorporate quizzing capabilities that enable us to further tailor the experience according to the learners’ real-time responses. Such personalization can be a powerful method for enhancing motivation deepening learning Bringing this to life involves a layered technical approach using , our best-in-class pedagogy-infused family of models, now integrated directly into . The first layer is a unique personalization pipeline that serves as the basis for the second layer of multiple content representations. Our starting point is a textbook PDF, although our approach could be used with other forms of source material. The personalization pipeline The Learn Your Way interface asks the learner to select their grade and interests (e.g., sports, music, food). The original source material is first re-leveled to the learner’s reported grade level, while maintaining the scope of its content. This is followed by the strategic replacement of generic examples with ones that are personalized to the learner’s reported interests. The resulting text serves as the basis for the generation of all the other representations, effectively propagating the personalization effect and setting up a pipeline for further personalization. Determine the appropriate width based on image_width For mobile images, use a default width Personalization of a generic text describing Newton’s law for two learner profiles (top) provides the basis for following representations of the content (bottom). Multiple representations of content Following the source personalization, we generate multiple representations of the content. For some content representations, such as mind maps and timelines, Gemini’s broad capabilities are used directly. Other features such as narrated slides, require more elaborate pipelines that weave together multiple specialized AI agents and tools to achieve an effective pedagogical result. Finally, specialized tasks, such as generating effective educational visuals, proved too challenging even for state-of-the-art general-purpose image models. To overcome this, we fine-tuned a dedicated model specifically for generating educational illustrations. The combination of a powerful base model, multi-step agentic workflows, and fine-tuned components allows us to generate a wide range of high-quality multimodal representations for learning. The Learn Your Way experience Our research comes to life in Learn Your Way. The interface brings together multiple, personalized representations of content including: (1) immersive text, (2) section-level quizzes, (3) slides & narration, (4) audio lessons, and (5) mind maps. Breaks the content up into digestible sections that are augmented with generated images and embedded questions. Put together, these transform passive reading into an active multimodal experience that follows learning science principles. Section-level quizzes : Promote active learning by allowing a user to interactively assess their learning, and uncover existing knowledge gaps. Slides & narration: Offers presentations that span the entire source material and include engaging activities like fill-in-the-blanks, as well as a narrated version, mimicking a recorded lesson. Provides simulated conversations, coupled with visual aids, between an AI-powered teacher and a student that models how a real learner might engage with the material, including the expression of misconceptions, which are clarified by the teacher. Organizes the knowledge hierarchically and allows learners to zoom in and out from the big picture to the details. The above representations give learners choice and are all adapted to their selected grade level and personal interests. Throughout the experience, the interactive quizzes provide dynamic feedback, guiding students to revisit specific content areas where they struggled. This marks our first steps towards true personalization. Determine the appropriate width based on image_width For mobile images, use a default width The Learn You Way interface provides easy access to multiple representations and practice opportunities. Pedagogical evaluation To evaluate Learn You Way's pedagogical performance, we transformed ten varied source materials from (a provider of free educational textbooks) to three different personalization settings. The source materials covered various subjects from history to physics. Three pedagogical subject matter experts then evaluated the transformed materials using pedagogical criteria, such as accuracy, coverage, and the learning science principles. Determine the appropriate width based on image_width For mobile images, use a default width Top pedagogical principles that guide the development and evaluation of new learning capabilities and experiences at Google The results were highly positive, with an average expert rating of 0.85 or higher across all pedagogical criteria. See the for more evaluation details. Determine the appropriate width based on image_width For mobile images, use a default width Expert ratings for the different transformations for four key criteria. An AI-powered learning tool is only valuable if it both effectively improves learning outcomes and students want to use it. Learn Your Way now serves as a research platform for us to conduct studies with partners around the world to explore how AI-powered transformations and personalization affects outcomes, and to ensure that what we build is effective and locally relevant Recently, we conducted a randomized controlled study with 60 students from the Chicago area, ages 15–18 and with similar reading levels. Participants were given up to 40 minutes to learn about adolescent brain development from a textbook, and randomly assigned to learn using Learn Your Way or a traditional digital PDF reader. We assessed students with a quiz immediately after the study session, and with a retention test 3–5 days later, using assessments designed by pedagogical experts to be a good measure of content comprehension. We also surveyed them about the learning experience, and to gain deeper insights beyond these quantitative metrics, each student participated in a 30-minute qualitative interview where they could share more nuanced feedback about their experience. The results were compelling and statistically significant. Here are the highlights. See the for more details. Positive learning outcomes: The Learn Your Way group scored, on average, 9% higher on the immediate assessment following the study session. Better long-term retention: Similarly, the Learn Your Way group scored 11% higher on the retention assessment 3-5 days later (78% vs. 67%). Positive user sentiment: 100% of students who used Learn Your Way reported that they felt the tool made them more comfortable taking the assessment, compared to 70% in the digital reader control group. 93% said they would want to use Learn Your Way for future learning, compared to just 67% for the digital reader. Valuable experience : Insights from the qualitative interviews revealed that students found great value in Learn Your Way. Determine the appropriate width based on image_width For mobile images, use a default width The group using Learn Your Way scored 9% higher on average on an immediate assessment than the group using a digital reader Experience Learn Your Way To give a concrete feel for the Learn Your Way interactive experience, today we are releasing example experiences on Google Labs A lesson on immune system challenges Learn about how to organize economies Discover what sociology is? The path forward Our findings suggest that generative AI can be used to build learning experiences that are not only more effective but also more empowering. By evolving the static textbook into an interactive artifact and giving students greater agency over they learn, we saw learning retention improve. This work is just the beginning of our exploration. We envision many more ways to tailor content, moving towards systems that continuously adapt to each learner's unique needs and progress. As we take our next steps towards personalized education, we will continue to ground our research in pedagogical principles, measuring the impact of AI on learning efficacy, so that in the future every student might have access to a high-quality, engaging learning experience that is custom built for them. Acknowledgements Shout out to our Google Research LearnLM team who have contributed to this work: Alicia Martín, Amir Globerson, Amy Wang, Anirudh Shekhawat, Anisha Choudhury, Anna Iurchenko, Avinatan Hassidim, Ayça Çakmakli, Ayelet Shasha Evron, Charlie Yang, Courtney Heldreth, Dana Oria, Diana Akrong, Hairong Mu, Ian Li, Ido Cohen, Komal Singh, Lev Borovoi, Lidan Hackmon, Lior Belinsky, Michael Fink, Preeti Singh, Rena Levitt, Shashank Agarwal, Shay Sharon, Sophie Allweis, Tracey Lee-Joe, Xiaohong Hao, Yael Gold-Zamir, Yishay Mor, and Yoav Bar Sinai. Special thanks to our executive champions: Niv Efron, Avinatan Hassidim, Yossi Matias and Ben Gomes. Education Innovation Other posts of interest A collaborative approach to image generation Human-Computer Interaction and Visualization Machine Intelligence September 30, 2025 AI as a research partner: Advancing theoretical computer science with AlphaEvolve Algorithms & Theory September 30, 2025 The anatomy of a personal health agent Health & Bioscience Machine Intelligence"
  },
  {
    "title": "VaultGemma: The world's most capable differentially private LLM",
    "url": "https://research.google/blog/vaultgemma-the-worlds-most-capable-differentially-private-llm/",
    "author": "Unknown",
    "date": "2025-09-12",
    "content": "VaultGemma: The world's most capable differentially private LLM September 12, 2025 Amer Sinha, Software Engineer, and Ryan McKenna, Research Scientist, Google Research We introduce VaultGemma, the most capable model trained from scratch with differential privacy. Technical report As AI becomes more integrated into our lives, building it with privacy at its core is a critical frontier for the field. Differential privacy (DP) offers a mathematically sound solution by adding calibrated noise to prevent memorization. However, applying DP to LLMs introduces trade-offs. Understanding these trade-offs is crucial. Applying DP noise alters traditional — rules describing performance dynamics — by reducing training stability (the model's ability to learn consistently without experiencing catastrophic events like loss spikes or divergence) and significantly increasing batch size (a collection of training examples sent to the model simultaneously for processing) and computation costs. Our new research, “ Scaling Laws for Differentially Private Language Models ”, conducted in partnership with Google DeepMind, establishes laws that accurately model these intricacies, providing a complete picture of the compute-privacy-utility trade-offs. Guided by this research, we’re excited to introduce VaultGemma, the largest (1B-parameters), open model trained from scratch with differential privacy. We are releasing the weights on technical report , to advance the development of the next generation of private AI. Understanding the scaling laws With a carefully thought-out experimental methodology, we aimed to quantify the benefit of increasing model sizes, batch sizes, and iterations in the context of DP training. Our work required making some simplifying assumptions to overcome the exponential number of combinations one might consider trying. We assumed that how well the model learns depends mostly on the \"noise-batch ratio” which compares the amount of random noise we add for privacy to the size of the data groups (batches) we use for training. This assumption works because the privacy noise we add is much greater than any natural randomness that comes from sampling the data. To establish a DP scaling law, we conducted a comprehensive set of experiments to evaluate performance across a variety of model sizes and noise-batch ratios. The resulting empirical data, together with known deterministic relationships between other variables, allows us to answer a variety of interesting scaling-laws–style queries, such as, “For a given compute budget, privacy budget, and data budget, what is the optimal training configuration to achieve the lowest possible training loss?” Determine the appropriate width based on image_width For mobile images, use a default width The structure of our DP scaling laws. We establish that predicted loss can be accurately modeled using primarily the model size, iterations and the noise-batch ratio, simplifying the complex interactions between the compute, privacy, and data budgets. Key findings: A powerful synergy Before diving into the full scaling laws, it’s useful to understand the dynamics and synergies between the compute budget, privacy budget, and data budget from a privacy accounting perspective — i.e., understand how these factors influence the noise-batch ratio for a fixed model size and number of iterations. This analysis is significantly cheaper to do as it does not require any model training, yet it yields a number of useful insights. For instance, increasing the privacy budget in isolation leads to diminishing returns, unless coupled with a corresponding increase in either the compute budget ( ) or data budget (tokens). For GIFs, use a default width For mobile images, use a default width Marginal benefit of increasing the privacy budget (epsilon) and the compute budget (batch size) in terms of their effect on the noise-batch ratio. To explore this synergy further, the visualization below shows how the optimal training configuration changes based on different constraints. As the privacy and compute budgets change, notice how the recommendation shifts between investing in a larger model versus training with larger batch sizes or more iterations. play silent looping video pause silent looping video Predicted training loss for different settings of data/privacy/compute budget, and a further detailed breakdown by the number of iterations, batch size, and model size. The plots show both the minimum achievable loss for different budget settings, along with the optimal hyper-parameter configurations. This data provides a wealth of useful insights for practitioners. While all the insights are reported in the paper, a key finding is that one should train a much smaller model with a much larger batch size than would be used without DP. This general insight should be unsurprising to a DP expert given the importance of large batch sizes. While this general insight holds across many settings, the optimal training configurations do change with the privacy and data budgets. Understanding the exact trade-off is crucial to ensure that both the compute and privacy budgets are used judiciously in real training scenarios. The above visualizations also reveal that there is often wiggle room in the training configurations — i.e., a range of model sizes might provide very similar utility if paired with the correct number of iterations and/or batch size. Applying the scaling laws to build VaultGemma models are designed with responsibility and safety at their core. This makes them a natural foundation for developing a production-quality, DP-trained model like VaultGemma. Algorithmic advancements: Training at scale The scaling laws we derived above represent an important first step towards training a useful Gemma model with DP. We used the scaling laws to determine both how much compute we needed to train a compute-optimal 1B parameter Gemma 2-based model with DP, and how to allocate that compute among batch size, iterations, and sequence length to achieve the best utility. One prominent gap between the research underlying the scaling laws and the actual training of VaultGemma was our handling of Poisson sampling , which is a central component of . We initially used a straightforward method of loading data in uniform batches but then switched to Poisson sampling to get the best privacy guarantees with the least amount of noise. This method posed two main challenges: it created batches of different sizes, and it required a specific, randomized order for processing the data. We solved this by using our recent work on , which allows us to process data in fixed-size batches — either by adding extra padding or trimming them — while still maintaining strong privacy protections. Armed with our new scaling laws and advanced training algorithms, we built VaultGemma, to date the largest (1B-parameters) open model fully pre-trained with differential privacy with an approach that can yield high-utility models. From training VaultGemma, we found our scaling laws to be highly accurate. The final training loss of VaultGemma was remarkably close to what our equations predicted, validating our research and providing the community with a reliable roadmap for future private model development. Determine the appropriate width based on image_width For mobile images, use a default width Performance comparison of VaultGemma 1B (differentially private) against its non-private counterpart (Gemma3 1B) and an older baseline (GPT-2 1.5B). The results quantify the current resource investment required for privacy and demonstrate that modern DP training yields utility comparable to non-private models from roughly five years ago. We also compare downstream performance of our model against its non-private counterpart across a range of standard academic benchmarks (i.e., E ). To put this performance in perspective and quantify the current resource investment required for privacy, we also include a comparison to an older similar-sized GPT-2 model, which performs similarly on these benchmarks. This comparison illustrates that today’s private training methods produce models with utility comparable to that of non-private models from roughly 5 years ago, highlighting the important gap our work will help the community systematically close. Finally, the model comes with strong theoretical and empirical privacy protections. Formal privacy guarantee In general, both the privacy parameters (ε, δ) and the privacy are important considerations when doing DP training, as these together determine what the trained model can learn. VaultGemma was trained with a -level DP guarantee of (ε ≤ 2.0, δ ≤ 1.1e-10), where a sequence consists of 1024 consecutive tokens extracted from heterogeneous data sources. Specifically, we used the same training mixture that was used to train the model, consisting of a number of documents of varying lengths. During pre-processing, long documents are split up and tokenized into multiple sequences, and shorter documents are packed together into a single sequence. While the sequence-level privacy unit was a natural choice for our training mixture, in situations where there is a clear mapping between data and users, user-level differential privacy would be a better choice. What does this mean in practice? Informally speaking, because we provide protection at the sequence level, if information relating to any (potentially private) fact or inference occurs in a single sequence, then VaultGemma essentially does not know that fact: the response to any query will be statistically similar to the result from a model that never trained on the sequence in question. However, if many training sequences contain information relevant to a particular fact, then in general VaultGemma will be able to provide that information. Empirical memorization To complement our sequence-level DP guarantee, we conduct additional tests of the empirical privacy properties of the trained model. To do so, we prompted the model with a 50-token prefix from a training document to see if it would generate the corresponding 50-token suffix. VaultGemma 1B shows no detectable memorization of its training data and successfully demonstrates the efficacy of DP training. VaultGemma represents a significant step forward in the journey toward building AI that is both powerful and private by design. By developing and applying a new, robust understanding of the scaling laws for DP, we have successfully trained and released the largest open, DP-trained language model to date. While a utility gap still exists between DP-trained and non-DP–trained models, we believe this gap can be systematically narrowed with more research on mechanism design for DP training. We hope that VaultGemma and our accompanying research will empower the community to build the next generation of safe, responsible, and private AI for everyone. Acknowledgements We'd like to thank the entire Gemma and Google Privacy teams for their contributions and support throughout this project, in particular, Peter Kairouz, Brendan McMahan and Dan Ramage for feedback on the blog post, Mark Simborg and Kimberly Schwede for help with visualizations, and the teams at Google that helped with algorithm design, infrastructure implementation, and production maintenance. The following people directly contributed to the work presented here (ordered alphabetically): Borja Balle, Zachary Charles, Christopher A. Choquette-Choo, Lynn Chua, Prem Eruvbetine, Badih Ghazi, Steve He, Yangsibo Huang, Armand Joulin, George Kaissis, Pritish Kamath, Ravi Kumar, Daogao Liu, Ruibo Liu, Pasin Manurangsi, Thomas Mesnard, Andreas Terzis, Tris Warkentin, Da Yu, and Chiyuan Zhang. Open Source Models & Datasets Security, Privacy and Abuse Prevention Technical report Other posts of interest A collaborative approach to image generation Human-Computer Interaction and Visualization Machine Intelligence September 30, 2025 AI as a research partner: Advancing theoretical computer science with AlphaEvolve Algorithms & Theory September 30, 2025 The anatomy of a personal health agent Health & Bioscience Machine Intelligence"
  },
  {
    "title": "Speculative cascades — A hybrid approach for smarter, faster LLM inference",
    "url": "https://research.google/blog/speculative-cascades-a-hybrid-approach-for-smarter-faster-llm-inference/",
    "author": "Unknown",
    "date": "2025-09-11",
    "content": "Speculative cascades — A hybrid approach for smarter, faster LLM inference September 11, 2025 Hari Narasimhan and Aditya Menon, Research Scientists, Google Research We introduce “speculative cascades”, a new approach that improves LLM efficiency and computational costs by combining speculative decoding with standard cascades. LLMs have transformed how we interact with technology, powering everything from advanced search capabilities to creative coding assistants. But this power comes at a cost: inference (the process of generating a response) can be slow and computationally expensive. As we deploy these models to more users, making them faster and less expensive without sacrificing quality is a critical challenge. One way to accomplish this would be to use , which aim to optimize LLM efficiency by strategically using smaller, faster models before engaging a larger, more expensive LLM. This approach involves a where the smaller model decides if it can handle a query or if it needs to pass the task to a more capable, but costlier, large model. The goal is to process as much as possible cheaply and quickly, only incurring the high cost of the large LLM for complex tasks that truly require its advanced capabilities, potentially yielding favorable cost-quality trade-offs. Cascades prioritize computational cost reduction and efficient resource allocation, while allowing for some variability in quality. Another approach, speculative decoding , optimizes an LLM’s latency and throughput without altering the final result . It achieves this by employing a smaller, faster \"drafter\" model to predict a sequence of future tokens. These speculated tokens are then quickly verified in parallel by the larger “target” model. If the draft is accepted, the large model effectively generates multiple tokens in a single step, greatly accelerating the process while guaranteeing that the final output is identical to what the large model would have produced on its own. This approach prioritizes speed and latency reduction, potentially at the cost of increased memory usage and less computational savings, since the larger model still performs substantial work. Faster Cascades via Speculative Decoding ”, we introduce “speculative cascades”, a new approach that combines the best of both cascades and speculative decoding. It delivers better LLM output quality at a lower computational cost than either technique alone by sometimes deferring to the smaller LLM for the sake of efficiency. We tested new speculative cascading techniques against standard cascading and speculative decoding baselines using models on various language tasks, including summarization, translation, reasoning, coding, and question answering. The results show that the proposed speculative cascades achieve better cost-quality trade-offs, often yielding higher speed-ups and better quality metrics compared to the baselines. To fully understand and appreciate the speculative cascades approach, we first compare cascades and speculative decoding with a simple example. Imagine you ask an LLM a straightforward question: Who is Buzz Aldrin? Let's say we have two models available to answer this: a small, fast \"drafter\" model and a large, powerful \"expert\" model. Here's how they might respond: Buzz Aldrin is an American former astronaut, engineer, and fighter pilot, best known as the second person to walk on the Moon. Edwin \"Buzz\" Aldrin, a pivotal figure in the history of space exploration, is an American former astronaut, engineer, and fighter pilot who is best known for being the second human to walk on the Moon. Both models provide excellent, factually correct answers, but they interpret the user's intent slightly differently. The small model delivers a quick, factual summary, while the large model provides a more formal, encyclopedic-style entry. Depending on the user's need — be it a fast fact or a detailed overview — either response could be considered ideal. The key is that they represent two distinct, equally valid styles. Now, let's see how the two main speed-up techniques handle this scenario. With cascades, the small \"drafter\" model gets the prompt first. If it's confident in its answer, it replies. If not, it defers the entire task to the large \"expert\" model. The small model generates its concise and correct answer. It checks its confidence and, finding it high, sends the response to the user. This works! We get a great answer quickly. But the process is sequential. If the small model been confident, we would have wasted time waiting for it to finish, only to then start the large model from scratch. This sequential \"wait-and-see\" approach is a fundamental bottleneck. With speculative decoding, the small model quickly drafts the first few tokens of the answer, and the large model verifies it in parallel, correcting the first mistake it finds. The small model drafts the beginning of its answer: [ The large model verifies this draft. Its own preferred first token is , the very first token is a mismatch. The entire draft is and the first token is replaced with . The process then repeats from this corrected point to generate the rest of the answer, but the initial speed advantage has been lost. Even though the small model produced a good answer, the requirement to match the large model token-by-token forces a rejection. We lose the speed benefit and end up with an answer that is not necessarily superior. While the above example uses a simple token matching rejection rule, in the full paper, we also include the potential for a \"probabilistic match\" that provides greater flexibility in the token-by-token comparison. Different goals, different trade-offs \" example reveals a fundamental difference between these two techniques, as summarized below: Determine the appropriate width based on image_width For mobile images, use a default width Determine the appropriate width based on image_width For mobile images, use a default width A visual representation of the trade-offs offered by standard cascades ( ) and speculative decoding ( ). In both graphs, the green star is the small, fast model (low cost, lower quality) and the red star is the large, slow model (high cost, higher quality). The dots in the left graph represent different trade-offs offered by cascades by varying its confidence threshold; the blue star in the right graph represents the trade-off offered by speculative decoding. Speculative cascades: Best of both worlds Speculative cascades combine the idea of tiered processing from standard cascades with the speedup mechanism of speculative decoding. It involves a smaller model generating a \"draft\" output that a larger model then quickly verifies in parallel. The key innovation is replacing the strict verification of speculative decoding with a flexible “deferral rule” This rule dynamically decides, on a token-by-token basis, whether to accept the small model's draft or defer to the large model. This avoids the sequential bottleneck of standard cascades while allowing the system to accept a good answer from the small model even if it doesn't exactly match the large model's preferred output. The small model drafts the beginning of its answer: [ Simultaneously, the large model evaluates the draft, providing its own scores. The crucial step: A flexible deferral rule looks at both outputs and decides whether a deferral is warranted. If the system decides , it accepts the small model's draft tokens. The process then efficiently repeats from this new point, drafting and verifying the next chunk of text until the answer is complete. The power of this method lies in its flexibility, as the deferral rule can be tailored to different needs. For example, we could tell the system to defer based on: A simple confidence check : Defer only if the small model isn't very confident in its own prediction. A comparative check : Defer if the large model is significantly more confident than the small model. A cost-benefit analysis : Defer only if the large model's confidence boost outweighs the \"cost\" of rejecting the small model's draft. A token-specific check : Given an \"approved list\" of the best next words according to the large model (its top-ranked tokens), we defer if the small model's drafted token is This ability to plug in different decision-making logic is what gives speculative cascades their unique blend of speed, quality, and adaptability. Determine the appropriate width based on image_width For mobile images, use a default width Block diagram illustrating a speculative cascade between a small and large model. As with standard speculative decoding, the drafting process involves auto-regressive sampling from the small drafter model. However, the verification process is different: it considers the combined output distribution of both the small and large models via a deferral rule, rather than solely relying on the large model's output. Below, we visualize the behaviour of speculative cascading versus speculative decoding on a prompt from the . The prompt asks, “Mary has 30 sheep. She gets 1 kg of milk from half of them and 2 kg of milk from the other half every day. How much milk does she collect every day?“ By carefully leveraging the small model's output on certain tokens, speculative cascading can reach a correct solution faster than regular speculative decoding. For GIFs, use a default width For mobile images, use a default width Comparison of speculative cascades and speculative decoding on a grade school math question from the GSM8K dataset The draft tokens are shown in yellow and the verified tokens in red. The speculative cascades approach generates the correct answer, and does so faster than speculative decoding. We tested speculative cascades on a range of benchmarks, including summarization, reasoning, and coding. The results show a clear advantage over speculative decoding. On a standard quality-versus-efficiency graph, speculative cascades consistently provide better trade-offs. This means for the same quality level as speculative decoding, our method is faster, i.e., generates more tokens per call to the larger model. Determine the appropriate width based on image_width For mobile images, use a default width Speculative cascades variants (blue and orange) achieve better quality-latency trade-offs compared to standard speculative decoding (green star) on math reasoning and summarization tasks. See Towards faster and smarter AI with speculative cascades As LLMs become more integrated into daily applications, optimizing their performance isn’t just a technical goal, it’s a practical necessity. By rethinking how cascades and speculative decoding can work together, speculative cascades provide a more powerful and flexible tool for developers. This hybrid approach allows for fine-grained control over the cost-quality balance, paving the way for applications that are both smarter and faster. Acknowledgements This work is a collaborative effort with Wittawat Jitkrittum, Ankit Singh Rawat, Seungyeon Kim, Neha Gupta and Sanjiv Kumar. We are grateful to Ananda Theertha Suresh and Ziteng Sun for their insightful discussions, and Yale Cong, Mark Simborg, and Kimberly Schwede for their help in crafting this blog. Natural Language Processing Security, Privacy and Abuse Prevention Other posts of interest A collaborative approach to image generation Human-Computer Interaction and Visualization Machine Intelligence September 30, 2025 AI as a research partner: Advancing theoretical computer science with AlphaEvolve Algorithms & Theory September 30, 2025 The anatomy of a personal health agent Health & Bioscience Machine Intelligence"
  },
  {
    "title": "Smarter nucleic acid design with NucleoBench and AdaBeam",
    "url": "https://research.google/blog/smarter-nucleic-acid-design-with-nucleobench-and-adabeam/",
    "author": "Unknown",
    "date": "2025-09-11",
    "content": "Smarter nucleic acid design with NucleoBench and AdaBeam September 11, 2025 Cory Y. McLean, Senior Staff Software Engineer, Google Research We developed an open-source software benchmark for nucleic acid sequence design, and introduced a novel algorithm, AdaBeam, that outperforms existing algorithms on 11 the 16 tasks, demonstrating superior scaling properties on long sequences and large predictors. AdaBeam design algorithm NucleoBench repository Designing new DNA and RNA sequences with specific therapeutic properties is a critical challenge in modern medicine. These molecules are the building blocks for next-generation treatments, from more precise gene therapies to more stable and effective . However, finding the right sequence is like searching for a single grain of sand on a vast beach. For instance, a small functional region of an RNA molecule called the can be one of over 2x10 possible sequences, making a brute-force search to optimize its function impossible. What if we could use AI to navigate this vast search space, drastically cutting down the time and cost of drug discovery? While various efforts have made in developing AI models that of a given nucleic acid sequence, there remains opportunity to innovate on the algorithms that use these models to optimal sequences. A lack of standardized evaluation hinders progress and prevents us from translating powerful predictive models into the best possible therapeutic molecules. To address this gap, in a research collaboration between Google Research and introduce NucleoBench , the first large-scale, standardized benchmark for comparing nucleic acid design algorithms. By running over 400,000 experiments across 16 distinct biological challenges, we've created a framework to rigorously evaluate and understand how different algorithms perform. The insights from this work enabled us to develop , a hybrid design algorithm that outperforms existing methods on 11 of the 16 tasks and scales more effectively to the large and complex models that are defining the future of AI in biology. We have made AdaBeam and all of our algorithm implementations freely available to spur further innovation. The core challenge The process of designing a new nucleic acid sequence using computers generally follows four steps: : Collect a high-quality dataset of sequences with the desired property (e.g., binding to a cancer-related protein). Train a predictive model : Use this data to train a model (often a neural network) that can predict the property from a DNA or RNA sequence. Generate candidate sequences : This is the crucial design step. Use an optimization algorithm to generate new sequences that the model predicts will have the highest possible score for the desired property. Validate candidates : Synthesize and test the most promising sequences in a wet lab to see if they work as predicted. [Optional]: Retrain the model on validation data. Determine the appropriate width based on image_width For mobile images, use a default width The typical workflow for computational nucleic acid design. In this work we focus on the design algorithms of step 3. At present, different research groups use different algorithms and test them on different tasks, making it impossible to know which methods are truly the best. Most existing benchmarks rely on algorithms like simulated annealing genetic algorithms , which were developed decades before modern deep learning and cannot take advantage of crucial information, like gradients, from the neural network models. To create a comprehensive and fair benchmark, we selected a diverse slate of gradient-free and gradient-based design algorithms. Gradient-free algorithms include well-established methods like directed evolution and simulated annealing, which are inspired by processes in evolution and physics, respectively. These algorithms treat the predictive AI model as a \"black box\", and test new sequences without needing to understand the model works internally. Their strength lies in their simplicity and broad applicability, but this means they potentially miss out on valuable clues from the model. Gradient-based design algorithms leverage the internal workings of neural networks and include more modern algorithms like . They use the model's gradients (i.e., the direction of steepest improvement) to intelligently guide the search for better sequences, but take longer to compute than just using the output of the neural network. To our knowledge, NucleoBench is the most comprehensive benchmark for nucleic acid design algorithms thus far and allows for a fair, apples-to-apples comparison between algorithms. We evaluated 9 different algorithms on the same 16 tasks with the same starting sequences, giving us unprecedented statistical power to draw meaningful conclusions. These tasks span a wide range of biological challenges, including: Controlling gene expression in specific cell types (e.g., liver or neuronal cells) Maximizing the binding of transcription factors (proteins that regulate genes) Improving the physical accessibility of chromatin for biomolecular interactions Predicting gene expression from very long DNA sequences using large-scale models like Speed (ms / example) Cell-type specific cis-regulatory activity How DNA sequences control gene expression from the same DNA molecule. Cell types include: precursor blood cells, liver cells, neuronal cells Transcription factor binding How likely a specific transcription factor will bind to a particular stretch of DNA Chromatin accessibility How physically accessible DNA is for interactions with other molecules Selective gene expression Prediction of gene expression Summary of design tasks in NucleoBench. *Model input length is 200K base pairs (bp), but only 256 bp are edited. We introduced ordered and unordered algorithms, staples from computer science, to test how fixing the order of sequence edits compares to a more flexible, random-order approach. We also created Gradient Evo, a novel hybrid that enhances the directed evolution algorithm by using model gradients to guide its mutations to independently evaluate how important gradients were for edit location selection versus selecting a specific edit. We also developed AdaBeam, a hybrid adaptive beam search algorithm that combines the most effective elements of unordered beam search with , a top-performing, non-gradient design algorithm. Adaptive search algorithms don't typically explore randomly; instead, their behavior changes as a result of the search to focus their efforts on the most promising areas of the sequence space. AdaBeam’s hybrid approach maintains a \"beam\", or a collection of the best candidate sequences found so far, and greedily expands on particularly promising candidates until they’ve been sufficiently explored. In practice, AdaBeam begins with a population of candidate sequences and their scores. In each round, it first selects a small group of the highest-scoring sequences to act as \"parents\". For each parent, AdaBeam generates a new set of \"child\" sequences by making a random number of random-but-guided mutations. It then follows a short, greedy exploration path, allowing the algorithm to quickly \"walk uphill\" in the fitness landscape. After sufficient exploration, all the newly generated children are pooled together, and the algorithm selects the absolute best ones to form the starting population for the next round, repeating the cycle. This process of adaptive selection and targeted mutation allows AdaBeam to efficiently focus on high-performing sequences. Computer-assisted design tasks pose difficult engineering problems, owing to the incredibly large search space. These difficulties become more acute as we attempt to design longer sequences, such as mRNA sequences, and use modern, large neural networks to guide the design. AdaBeam is particularly efficient on long sequences by using fixed-compute probabilistic sampling instead of computations that scale with sequence length. To enable AdaBeam to work with large models, we reduce peak memory consumption during design by introducing a trick we call “gradient concatenation.” However, existing design algorithms that don’t have these features have difficulty scaling to long sequences and large models. Gradient-based algorithms are particularly affected. To facilitate a fair comparison, we limit the length of the designed sequences, even though AdaBeam can scale longer and larger. For example, even though the DNA expression prediction model runs on ~200K nucleotide sequences, we limit design to just 256 nucleotides. Determine the appropriate width based on image_width For mobile images, use a default width Summary of design algorithms in NucleoBench. Below the solid line are design algorithms devised in this work. We evaluate each design algorithm based on the final fitness score of the sequence each produced. The fitness score is defined as how well the sequence performed on the biological task according to the predictive model. To ensure fairness, we ran over 400,000 experiments where each design algorithm was given a fixed amount of time and the exact same 100 starting sequences for each task. We also measured the convergence speed, tracking how quickly each algorithm found its best solution, as faster algorithms save valuable time and computational resources. We characterized performance variability by measuring how much an algorithm's final score was influenced by random chance versus its starting sequence. We quantified the effect of algorithmic randomness by re-running experiments with five different random seeds. To assess the impact of the starting point, we analyzed the variance in final scores across the 100 identical start sequences given to each design algorithm. We used a to investigate whether \"intrinsically difficult start sequences\", or sequences that are hard for all algorithms to optimize, exist. To assess the distribution of performance ranks, we compared the final performance for each of the nine algorithms across every experiment in the NucleoBench benchmark for each unique combination of a task and a starting sequence. A rank-based \"order score\" from 0 to 8 was then assigned, with 0 going to the best-performing algorithm, 1 to the second-best, and so on. Each violin shape is constructed by aggregating all the rank scores a single algorithm received across the 400,000+ experiments, with the width of the violin at any point showing how frequently that algorithm achieved a particular rank. Determine the appropriate width based on image_width For mobile images, use a default width The distribution of final scores for each algorithm. X-axis is the design algorithm, y-axis is the aggregate order score. Order scores are determined by assigning an integer [0, 9] for each (task, start sequence, design algorithm) tuple according to the performances of all the final sequences for that (task, start sequence) pair. 0 is the top performer. Aggregate scores are computed by averaging over all such scores. Gradient-based methods were the reigning champions amongst existing methods. However, we found that AdaBeam outperformed them, demonstrating that relying on gradients is not the only path to top-tier performance and scalability. AdaBeam improves upon previous methods in several key ways: : It replaces AdaLead’s sampling step with a faster calculation, doubling its speed on long sequences. Smart Exploration : It uses a significantly more effective \"unordered\" approach to deciding where to edit a sequence. Advanced Engineering : It uses gradient concatenation to substantially reduce memory usage, enabling application to massive models like Enformer. Across the 16 tasks in NucleoBench, AdaBeam was the best-performing algorithm 11 times. It also proved to be one of the fastest to converge on a high-quality solution, demonstrating superior scaling properties that are essential for tackling the next generation of AI challenges in biology. Future directions benchmark reveals the importance of rigorous, standardized evaluation and uncovers surprising findings, such as the critical impact of the initial sequence and the ineffectiveness of some established algorithm features. However, significant challenges remain. The best gradient-based methods still struggle to scale to the largest models and longest sequences, and substantial scalability gains can be realized through better software engineering. While our new algorithm, AdaBeam, sets a new state-of-the-art, future work must focus on algorithms that adhere to biological constraints and improve scalability. A core principle of our work is a commitment to biosafety and responsible innovation. While AdaBeam represents a step forward for biological sequence design, it only improves the optimization according to a pre-existing predictive model. In other words, it is an optimizer, not an originator; the algorithm can only design sequences to maximize a goal defined by a user-provided predictive model. By releasing AdaBeam as an open-source tool, we empower researchers while ensuring the “human-in-the-loop” remains central to the design of biological molecules. Algorithms like AdaBeam can help scientists design more effective mRNA vaccines, create safer CRISPR gene therapies, and develop novel treatments for a wide range of diseases, bringing the promise of AI-driven drug discovery closer to reality. This work represents a collaboration between Joel Shor (Move37 Labs), Erik Strand (Move37 Labs, MIT), and Cory Y. McLean (Google Research). We thank Sager Gosai, Daniel Friedman, Anna Lewis, Vikram Agarwal, and Michael Brenner for their guidance, discussions, and support throughout this project. Health & Bioscience Machine Intelligence Open Source Models & Datasets AdaBeam design algorithm NucleoBench repository Other posts of interest A collaborative approach to image generation Human-Computer Interaction and Visualization Machine Intelligence September 30, 2025 The anatomy of a personal health agent Health & Bioscience Machine Intelligence September 25, 2025 Towards better health conversations: Research insights on a “wayfinding” AI agent based on Gemini Health & Bioscience Human-Computer Interaction and Visualization"
  },
  {
    "title": "Accelerating scientific discovery with AI-powered empirical software",
    "url": "https://research.google/blog/accelerating-scientific-discovery-with-ai-powered-empirical-software/",
    "author": "Unknown",
    "date": "2025-09-09",
    "content": "Accelerating scientific discovery with AI-powered empirical software September 9, 2025 Lizzie Dorfman, Product Manager, and Michael Brenner, Research Scientist, Google Research Our new AI system helps scientists write empirical software, achieving expert-level results on six diverse, challenging problems. Interactive website with tree visualizations In scientific research, thoroughly evaluating hypotheses is essential to developing more robust and comprehensive answers, but the required work forms a bottleneck, hindering the pace of discovery. In particular, much of modern scientific research depends on computational experiments to model, simulate, and analyze complex phenomena. Here, hypothesis evaluation often requires creating custom software, a slow and challenging task. Given the increasing capability of large language models (LLMs) to perform traditional coding tasks , we wondered if they could similarly generate high-quality custom software for evaluating and iteratively improving scientific hypotheses. Today we are releasing a paper describing an \" AI system designed to help scientists write expert-level empirical software . Taking as input a well-defined problem and a means of evaluation, our system acts as a systematic code-optimizing research engine: it can propose novel methodological and architectural concepts, implement them as executable code and empirically validate their performance. It then searches and iterates through thousands of code variants, using to optimize performance. We tested our system using six benchmarks representing distinct multidisciplinary challenges, spanning the fields of genomics, public health, geospatial analysis, neuroscience, time-series forecasting, and numerical analysis. Our system achieves expert-level performance across all of these benchmarks. Empirical software and scorable tasks Scientific research is inherently iterative, often requiring researchers to test dozens or hundreds of models or parameters to achieve a breakthrough. Even for scientists who are experienced programmers, coding, debugging, and optimizing software is incredibly time-consuming. Manually coding each new idea is slow and inefficient, making systematic exploration of potential solutions practically impossible. At the heart of our system lies the foundational concept of empirical software. Unlike conventional software, which is often judged by functional correctness alone, empirical software is designed with a primary objective: to maximize a predefined quality score. A problem or challenge that can be effectively addressed and solved through the application of empirical software is termed a scorable task. These scorable tasks are prevalent across science, applied mathematics, and engineering. The input to our system is a scorable task, which includes a problem description, a scoring metric, and data suitable for training, validation, and evaluation. A user can also provide context, such as ideas from external literature, or directives for methodologies to prioritize. The system then generates research ideas, including programmatic reproduction, optimization, and recombination of known methods, leading to novel and highly performant approaches. Ideas are implemented as executable code and the system uses a tree search strategy with an upper confidence bound (inspired by ) to create a tree of software candidates and decide which candidates warrant further exploration. It then uses an LLM to rewrite the code to attempt to improve its quality score, and can exhaustively and tirelessly carry out solution searches at an unprecedented scale, identifying high-quality solutions quickly, reducing exploration time from months to hours or days. Its outputs, as coded solutions, are verifiable, interpretable and reproducible. Determine the appropriate width based on image_width For mobile images, use a default width Schematic of the algorithm that feeds a scorable task and research ideas to an LLM, which generates evaluation code in a sandbox. This code is then used in a tree search, where new nodes are created and iteratively improved using the LLM. Demonstrated effectiveness The evaluation of code generating AI systems has historically focused on tasks derived from competitive programming or software engineering, which, while valuable, fail to capture the full spectrum of challenges inherent in scientific discovery. We demonstrate proficiency not merely in writing syntactically correct code, but in generating novel solutions to six diverse and challenging benchmark problems that push the boundaries of current computational methods and human expertise. The diversity of these benchmarks allows us to collectively assess proficiency in areas such as zero-shot generalization high-dimensional signal processing uncertainty quantification semantic interpretation of complex data, and systems-level modeling . The top scoring solutions to each of these benchmark problems are openly available for anyone interested in reproducing our results, including as an interactive website to explore the full candidate solution trees. Genomics: Batch integration of single cell RNA sequencing data Single-cell RNA sequencing (scRNA-seq) is a powerful technology that provides a high-resolution view of gene expression at the individual cell level. A major challenge required to jointly analyze many disparate datasets is to remove complex present across samples while preserving true biological signals. Nearly 300 tools exist to perform batch integration of scRNA-seq data, and multiple benchmarks have been developed for assessing metrics of batch effect removal and conservation of biological variability. Using the V2.0.0 batch integration benchmark , which combines 13 metrics into one overall score, our system discovered 40 novel methods that outperformed top expert-developed methods. The highest-scoring solution achieved a 14% overall improvement over the best published method ( ) by successfully combining two existing methods (ComBat and Determine the appropriate width based on image_width For mobile images, use a default width Overall leaderboard for OpenProblems benchmark v2.0.0 non-control methods. In blue are results from our system with and without recombination of ideas, and Gemini Deep Research to enlarge image. Public health: Prediction of U.S. COVID-19 hospitalizations The primary U.S. benchmark for COVID-19 forecasting is the COVID-19 Forecast Hub (CovidHub), a large collaborative effort coordinated by the Centers for Disease Control and Prevention (CDC). CovidHub attracts competitive and methodologically diverse submissions from dozens of expert-led teams. Their task is to forecast new COVID-19 hospitalizations across all the U.S. states and its territories for up to a month ahead. These forecasts are evaluated using average weighted interval score (WIS), which assesses the quality of probabilistic forecasts by summarizing a model's performance across all locations for every weekly prediction over the season. Individual submissions are then aggregated into the CovidHub Ensemble model , which is considered the gold standard in the U.S. for forecasting COVID-19 hospitalizations. Our system generated 14 models that outperform the official CovidHub Ensemble. Determine the appropriate width based on image_width For mobile images, use a default width Time-series leaderboard showing weekly forecasting performance for teams participating in the COVID-19 Forecast Hub, ordered by absolute average WIS (number within each cell). Scores are aggregated across 52 jurisdictions and four forecast horizons. The cell’s background color visualizes the performance relative to the CovidHub-ensemble, with blue indicating a lower (better) WIS and red indicating a higher (worse) WIS. Our method, the top row of the table (Google Retrospective) outperforms CovidHub-ensemble. to enlarge image. Geospatial analysis: Segmentation of remote sensing images Semantic segmentation of high-resolution images is a common problem in geospatial analysis, and is essential for diverse applications, ranging from monitoring land use assessing the environmental impacts of human activity managing natural disasters . This task, which involves accurately assigning class labels to individual pixels in an image, requires a model to develop a spatial and contextual understanding of the scene, identifying not just what objects are present, but precisely where their boundaries lie. dense labeling remote sensing dataset (DLRSD) benchmark, which evaluates methods using a mean intersection over union (mIoU), the top three solutions generated by our system are slightly better than current state of the art, with mIoU greater than 0.80. All three solutions build upon existing models, libraries and strategies. Two leverage standard UNet++ and U-Net models but paired with powerful encoders pre-trained on . The third uses , a state of the art -based architecture. All three employ extensive test-time augmentation Determine the appropriate width based on image_width For mobile images, use a default width The input to remote sensing segmentation models is an image ( ), and the output is a new image, often called a segmentation mask, where each pixel is assigned a specific class label. The is the true mask as provided by the DLRSD benchmark. The is segmentation masks generated using our system's top scoring solution. High-scoring segmentation models will have close visual similarity to the ground truth mask. Neuroscience: Whole-brain neural activity prediction We applied our method to the Zebrafish Activity Prediction Benchmark (ZAPBench), a recent benchmark for forecasting the activity of over 70,000 neurons across an entire vertebrate brain. Our system discovered a novel time-series forecasting model that achieved state-of-the-art performance, surpassing all existing baselines. This includes a computationally intensive, video-based model that forecasts 3D volumes and was the previous top performing solution. As a proof of concept, we also demonstrated that our system can design hybrid models that incorporate a biophysical neuron simulator ( ), paving the way for more interpretable predictive models. While each of these examples is compelling in its own right, our system to generate empirical software is striking in its generalizability. We additionally evaluated our system in the context of mathematics on the task of numerical evaluation of difficult integrals. In this task, our system generated a solution that correctly evaluated 17 out of 19 held-out numerical method failed. Lastly, we evaluated our system on the general problem of time series forecasting, using the General Time Series Forecasting Model Evaluation (GIFT-Eval), a benchmark derived from 28 datasets spanning seven diverse domains, with 10 different frequencies, from seconds to years. Our system successfully created a unified, general purpose forecasting library from scratch, by hill climbing with a single code on the average mean absolute scaled error on the entire GIFT-Eval dataset. See the for more details. Recent advances in LLMs have already given researchers worldwide new ways to easily engage with knowledge and ideas , and LLMs are increasingly being pursued as a means of automating the rote and toilsome aspects of scientific research. We explored whether LLMs could be useful for the ubiquitous, essential, and highly time-consuming task of producing custom software for evaluating and iteratively improving scientific hypotheses, motivated by the possibility of a future where scientists can easily, rapidly, and systematically investigate hundreds or thousands of potential solutions to the questions and problems that motivate their research. Our system quickly generates expert-level solutions reducing the time required for exploration of a set of ideas from months to hours or days. This promises to save significant time for scientists, from students to professors, to focus on truly creative and critical challenges, and to continue to define and prioritize the fundamental research questions and societal challenges that scientific research can help address. Acknowledgements We thank and acknowledge the contributions from all of the co-authors of the manuscript. Thanks to Shibl Mourad, John Platt, Erica Brand, Katherine Chou, Ronit Levavi Morad, Yossi Matias, and James Manyika for their support and leadership. Interactive website with tree visualizations Other posts of interest A collaborative approach to image generation Human-Computer Interaction and Visualization Machine Intelligence September 30, 2025 AI as a research partner: Advancing theoretical computer science with AlphaEvolve Algorithms & Theory September 30, 2025 The anatomy of a personal health agent Health & Bioscience Machine Intelligence"
  }
]